{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1flPN_4P5bkEa_8d140GXXNG0RIbkscD2",
      "authorship_tag": "ABX9TyNuU4boPdkjW0vOtElwDNJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tawanss/chatbot_project/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hajwsZnJU4LU",
        "outputId": "12ba30ba-2f81-49e7-d0ed-71417b2a1100"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf2\n",
            "Successfully installed pypdf2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfpGa3W-VCje",
        "outputId": "da761653-b59f-4a1b-c366-4ef511188361"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.30 (from langchain)\n",
            "  Downloading langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.30->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (4.12.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.2.13-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.30-py3-none-any.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.13 langchain-core-0.2.30 langchain-text-splitters-0.2.2 langsmith-0.1.99 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmcPw7qcVKxQ",
        "outputId": "eb94c2b2-3db9-40c4-9928-3aac5b35933e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.13)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.30)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USIGNjTWiHoL",
        "outputId": "feae3ca8-b9f6-4200-a6ba-9fb3ae6879a7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZF_RHVJi-HT",
        "outputId": "71cd6859-9b5a-4ab5-c47c-fe3700f1e96b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VtCJzhSxUQol"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import google.generativeai as genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVbI9pcobEqo",
        "outputId": "3974a59b-7dbe-4856-d016-712f2d4cef00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"AIzaSyD3OGXFD39772EiJtMQUTi1Tei7-QUtIxc\"\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "j-bE7fSNVczs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ฟังก์ชันสำหรับอ่านไฟล์ PDF\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# ฟังก์ชันสำหรับแบ่งข้อความเป็นชิ้นเล็กๆ\n",
        "def split_text(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# ฟังก์ชันสำหรับสร้าง vector store\n",
        "def create_vector_store(chunks):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
        "    return vector_store\n",
        "\n",
        "# ฟังก์ชันสำหรับค้นหาข้อมูลที่เกี่ยวข้อง\n",
        "def retrieve_relevant_chunks(vector_store, query, k=3):\n",
        "    relevant_chunks = vector_store.similarity_search(query, k=k)\n",
        "    return [chunk.page_content for chunk in relevant_chunks]\n",
        "\n",
        "# ฟังก์ชันสำหรับสร้างคำถามและรับคำตอบจาก Gemini\n",
        "def ask_gemini(query, relevant_chunks):\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    prompt = f\"\"\"Context information is below.\n",
        "    ---------------------\n",
        "    {' '.join(relevant_chunks)}\n",
        "    ---------------------\n",
        "    Given the context information and not prior knowledge, answer the question: {query}\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "YFxO5caFYnp0"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = read_pdf(\"/content/drive/MyDrive/อ้างอิง chatbot/when-life-gives-you-lemons-make-cherryade-converting-397dlsoh (2).pdf\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Q4qzSUbVbl",
        "outputId": "a936a70e-811c-4a85-ba38-4220d2d421a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When Life Gives You Lemons\n",
            " , Make Cherryade\n",
            " :\n",
            "Converting Feedback from Bad Responses into Good Labels\n",
            "Weiyan Shiy\n",
            "Meta AI & Columbia UniversityEmily Dinan\n",
            "Meta AIKurt Shuster\n",
            "Meta AI\n",
            "Jason Weston\u0003\n",
            "Meta AIJing Xu\u0003\n",
            "Meta AI\n",
            "Abstract\n",
            "Deployed dialogue agents have the potential\n",
            "to integrate human feedback to continuously\n",
            "improve themselves. However, humans may\n",
            "not always provide explicit signals when the\n",
            "chatbot makes mistakes during interactions. In\n",
            "this work, we propose J UICER , a framework to\n",
            "make use of both binary and free-form textual\n",
            "human feedback. It works by: (i) extending\n",
            "sparse binary feedback by training a satisfac-\n",
            "tion classiﬁer to label the unlabeled data; and\n",
            "(ii) training a reply corrector to map the bad\n",
            "replies to good ones. We ﬁnd that augment-\n",
            "ing training with model-corrected replies im-\n",
            "proves the ﬁnal dialogue model, and we can\n",
            "further improve performance by using both\n",
            "positive and negative replies through the re-\n",
            "cently proposed D IRECTOR model.\n",
            "1 Introduction\n",
            "Existing dialogue models are primarily trained on\n",
            "human-human conversations (Conneau et al., 2019;\n",
            "Baumgartner et al., 2020; Smith et al., 2020). As\n",
            "dialogue agents become increasingly powerful and\n",
            "carry substantial conversations with humans (Shus-\n",
            "ter et al., 2022b), it becomes pressing to have the\n",
            "models learn from dialogue successes and failures\n",
            "in the wild, and hence improve after deployment.\n",
            "Prior work has studied how to collect and learn\n",
            "from feedback in human-model dialogues (Li et al.,\n",
            "2016a,b; Hancock et al., 2019; Xu et al., 2022). But\n",
            "most existing methods were proposed under set-\n",
            "tings where either feedback can be obtained when-\n",
            "ever needed or all turns are annotated with human\n",
            "feedback. For instance, Xu et al. (2022) introduced\n",
            "a dataset with all turns annotated by crowdworkers\n",
            "with three types of feedback: (1) binary thumbs\n",
            "up/down; (2) free-form textual feedback on what\n",
            "went wrong; (3) gold corrections on what the bot\n",
            "should have said instead. Unfortunately, annota-\n",
            "yWork done when interning at Meta AI.\n",
            "* Equal contribution.tions such as thumb ups/downs and gold correc-\n",
            "tions are often sparse in real-life deployment set-\n",
            "tings. For example, human conversationalists give\n",
            "thumbs up/down to bot messages in conversations\n",
            "with the deployed BlenderBot3 model around 5-6%\n",
            "of the time (Shuster et al., 2022b). On the other\n",
            "hand, human conversationalists may express their\n",
            "dissatisfaction with bad responses and explain what\n",
            "went wrong more naturally in free-form textual\n",
            "feedback as part of the conversation, rather than\n",
            "providing the exact gold corrections to those bot\n",
            "responses. Therefore, in this paper we study how to\n",
            "utilize sparse binary and gold correction feedback,\n",
            "and relatively dense free-form textual feedback to\n",
            "improve dialogue models during deployment.\n",
            "In this work, we introduce JUICER , a framework\n",
            "to “squeeze the juice” out of the sparse human feed-\n",
            "back in human-model conversations to improve the\n",
            "dialogue models after deployment. JUICER con-\n",
            "sists of four steps: (1) we ﬁrst train a binary satis-\n",
            "faction classiﬁer and a reply corrector on existing\n",
            "binary feedback and gold corrections; (2) we then\n",
            "use the satisfaction classiﬁer to label all the bot re-\n",
            "sponses that are missing human labels; (3) next we\n",
            "use the reply corrector to correct bad bot responses\n",
            "(lemons\n",
            " ) into good ones, conditioning on hu-\n",
            "man textual feedback; (4) ﬁnally we augment the\n",
            "training data with the new good responses (cher-\n",
            "ryade\n",
            " ) and re-train our ﬁnal dialogue models.\n",
            "To evaluate JUICER on state-of-the-art chatbots\n",
            "in such a setting, we thus construct a new sparse\n",
            "sampled version of the existing FITS dataset from\n",
            "Xu et al. (2022), which consists of fully annotated\n",
            "human-model conversations between users and ex-\n",
            "isting state-of-the-art internet-augmented models\n",
            "such as BlenderBot 2 (Komeili et al., 2021; Xu\n",
            "et al., 2021) and SeeKeR (Shuster et al., 2022a).\n",
            "We explore a variety of methods to take advan-\n",
            "tage of limited human feedback at each step of the\n",
            "JUICER framework. Our main results are:arXiv:2210.15893v1  [cs.CL]  28 Oct 2022•We show that free-form textual feedback is\n",
            "a very useful signal for improving the per-\n",
            "formance of both a satisfaction classiﬁer to\n",
            "identify good and bad responses, and a reply\n",
            "corrector to generate better corrections.\n",
            "•Augmenting training data with reply-\n",
            "corrector -generated corrections works\n",
            "better than only training with existing gold\n",
            "corrections.\n",
            "•Models such as DIRECTOR (Arora et al.,\n",
            "2022) that utilize both gold/predicted good\n",
            "and bad responses further improves the ﬁnal\n",
            "dialogue model. Our ﬁnal best models out-\n",
            "perform the baseline BlenderBot 2 model or\n",
            "using D IRECTOR alone.\n",
            "2 Related Work\n",
            "Many recent works have studied how to align lan-\n",
            "guage models with human feedback (Nakano et al.,\n",
            "2021; Ouyang et al., 2022; Scheurer et al., 2022;\n",
            "Saunders et al., 2022; Schick et al., 2022). For in-\n",
            "stance, InstructGPT (Ouyang et al., 2022) was ﬁne-\n",
            "tuned using feedback from labelers who ranked\n",
            "model outputs. Scheurer et al. (2022) ﬁne-tuned\n",
            "GPT-3 and InstructGPT on 100 examples of free-\n",
            "form textual feedback from humans to improve\n",
            "summarization tasks and found that only the larger\n",
            "models such as GPT-3 (175B) (Brown et al., 2020)\n",
            "can generate accurate reﬁnements using feedback.\n",
            "Saunders et al. (2022) ﬁne-tuned large language\n",
            "models to generate self-critiques for summarization\n",
            "tasks to assist human annotators, and continued to\n",
            "reﬁne the models on feedback. In this work, we\n",
            "focus on improving dialogue agents given various\n",
            "human feedback signals (binary, free-form natural\n",
            "language and gold corrections) and compare our\n",
            "methods to Scheurer et al. (2022).\n",
            "Existing works have also studied how to correct\n",
            "language model output. For instance, Elgohary\n",
            "et al. (2021) proposed a model to understand natu-\n",
            "ral language feedback and produce a series of edits\n",
            "to correct a text-to-SQL semantic parser. Tandon\n",
            "et al. (2022) trained a memory-augmented corrector\n",
            "to convert feedback to edits and ﬁx model outputs\n",
            "for a script generation task. Some recent large lan-\n",
            "guage model research can also repair generations\n",
            "given human feedback (Scheurer et al., 2022; Saun-\n",
            "ders et al., 2022).\n",
            "Past research has also explored how to integrate\n",
            "feedback into dialogue agents (Li et al., 2016a,b;Hancock et al., 2019; Shuster et al., 2020; Xu et al.,\n",
            "2022). Li et al. (2016a) investigated how to im-\n",
            "prove the chatbot’s question-answering ability with\n",
            "general textual feedback in a reinforcement learn-\n",
            "ing setting. Hancock et al. (2019) developed a\n",
            "self-feeding chatbot that can construct new exam-\n",
            "ples from existing human-bot conversations and\n",
            "ask for feedback when necessary to improve itself.\n",
            "Xu et al. (2022) proposed a dataset with internet-\n",
            "augmented dialogues, where each turn is annotated\n",
            "with human feedback, and they found that continu-\n",
            "ously retraining the model on binary feedback after\n",
            "deployment is helpful. Our work focuses on con-\n",
            "verting bad responses into good ones to augment\n",
            "the data and learn from feedback about failures.\n",
            "3 Human Feedback Setting\n",
            "As illustrated in the dialogue example in Figure 1,\n",
            "we consider a deployed system where one can col-\n",
            "lect three types of feedback:\n",
            "(1)binary feedback , where the human conversa-\n",
            "tionalist explicitly likes (\n",
            " ) or dislikes (\n",
            ") a bot response;\n",
            "(2)free-form textual feedback , where the hu-\n",
            "man explains conversationally what was\n",
            "wrong when they dislike a response (e.g.,\n",
            "“That’s a quick topic change! Let’s continue to\n",
            "talk about fruit, perhaps fruit drinks?” );\n",
            "(3)gold correction , where the human conversa-\n",
            "tionalist suggests an alternative reply the bot\n",
            "should have said, (e.g., “I like watermelons\n",
            "too! They tastes great in drinks. ” ).\n",
            "In a deployment setting, it is unnatural to ask users\n",
            "to always click the thumbs up and down and pro-\n",
            "vide gold corrections whenever the bot makes a\n",
            "mistake. Instead, users tend to provide free-form\n",
            "textual feedback on what was wrong in their dia-\n",
            "logue response to express dissatisfaction when the\n",
            "bot makes errors (See and Manning, 2021). There-\n",
            "fore many responses may be missing binary feed-\n",
            "back (Shuster et al., 2022b). In this paper, we con-\n",
            "sider a sparse thumbs up/down signal and sparse\n",
            "gold correction signal setting, but a dense free-form\n",
            "textual feedback signal (i.e., mistakes are followed\n",
            "by textual feedback). After collecting conversa-\n",
            "tions with these feedback signals, we can consider\n",
            "methods to utilize them to improve the dialogue\n",
            "model.Figure 1: Our J UICER model. During deployment, we collect three types of human feedback: (1) binary thumbs up\n",
            "and down; (2) free-form textual feedback on what was wrong with the reply ( “That’s a quick topic change! Let’s\n",
            "continue to talk about fruit, perhaps fruit drinks?” ); (3) gold corrections of poor replies ( “I like watermelons too!\n",
            "They tastes great in drinks. ” ). In J UICER , (1) we ﬁrst train a satisfaction classiﬁer and a reply corrector on existing\n",
            "feedback, (2) we then use the satisfaction classiﬁer to predict binary satisfaction labels for the un-annotated turns,\n",
            "(3) next we use the reply corrector to convert the bad replies to good ones, (4) ﬁnally we collect the good and bad\n",
            "replies including corrections and re-train the ﬁnal dialogue model to improve it with human feedback.\n",
            "4 The JUICER Method\n",
            "Figure 1 shows the overview of our framework\n",
            "JUICER to incorporate limited human feedback to\n",
            "improve the deployed dialogue model. The frame-\n",
            "work consists of training a satisfaction classiﬁer , a\n",
            "reply corrector , and eventually the ﬁnal dialogue\n",
            "model itself. We deﬁne the notation here. For a\n",
            "given bot reply: (1)\n",
            " denotes un-annotated turns;\n",
            "(2)\n",
            " and\n",
            " : annotated as good or bad responses\n",
            "by users, as deﬁned before; (3)\n",
            " : predicted as\n",
            "bad by the satisfaction classiﬁer ; (4)\n",
            " : predicted\n",
            "as good by the satisfaction classiﬁer .\n",
            "JUICER involves four steps, summarized here:\n",
            "1.Step 1. Train two supervised models: a sat-\n",
            "isfaction classiﬁer to detect good and bad\n",
            "replies, and a reply corrector to correct the\n",
            "bad replies to good ones.\n",
            "2.Step 2. Apply the satisfaction classiﬁer to\n",
            "predict binary labels for all replies missingbinary feedback (\n",
            " !\n",
            " or\n",
            " ). After\n",
            "this step, each bot reply has a label.\n",
            "3.Step 3. Use the reply corrector to convert the\n",
            "bad replies (those that are either disliked by\n",
            "human users or are predicted as bad by the\n",
            "satisfaction classiﬁer inStep 2 to good replies\n",
            "(\n",
            "!\n",
            " ,\n",
            "!\n",
            " ).\n",
            "4.Step 4. Re-train the ﬁnal dialogue model by\n",
            "augmenting the training data with the good\n",
            "(\n",
            " +\n",
            " ) and bad (\n",
            " +\n",
            " ) replies de-\n",
            "rived from human feedback and the predic-\n",
            "tions from the previous steps.\n",
            "Now we describe each step in more detail.\n",
            "4.1 Step 1: Train satisfaction classiﬁer and\n",
            "reply corrector on existing feedback\n",
            "We ﬁrst train two models: (1) a satisfaction classi-\n",
            "ﬁer, and (2) a reply corrector in order to build anaugmented training set in later steps. In our experi-\n",
            "ments, both models are trained with human-labeled\n",
            "data which come from the FITS task (Xu et al.,\n",
            "2022), described further in Section 5.1.1.\n",
            "(1a) Satisfaction classiﬁer The training target of\n",
            "thesatisfaction classiﬁer is a binary satisfaction\n",
            "label (\n",
            " or\n",
            " ). For the input to the classiﬁer,\n",
            "we experimented with two variants: (1) the context\n",
            "+ the bot reply to be labeled, and (2) the context\n",
            "+ the bot reply to be labeled + the next human re-\n",
            "sponse. As shown in the example in Figure 1, when\n",
            "the ﬁrst bot reply is given a thumbs-up, the next\n",
            "human response is a natural continuation of the con-\n",
            "versation (e.g., “I also like watermelons” ); when\n",
            "the bot reply is disliked (the second bot reply), the\n",
            "next human response is free-form textual feedback\n",
            "on what went wrong (e.g., “That’s a quick topic\n",
            "change! Let’s continue to talk about fruit, perhaps\n",
            "fruit drinks?” ). Hence, the next human response\n",
            "can be indicative of the quality of its preceding\n",
            "bot reply, and we include it in the input. In our\n",
            "experiments, the satisfaction classiﬁer is trained\n",
            "by ﬁne-tuning a 311M-parameter transformer pre-\n",
            "trained on pushshift.io Reddit data (Baumgartner\n",
            "et al., 2020).\n",
            "(1b) Reply corrector The input to the reply cor-\n",
            "rector is the context + the bad bot reply to correct\n",
            "+ the next human free-form textual feedback on\n",
            "what went wrong. The training target is the cor-\n",
            "rection to the bad reply which can be either (1)\n",
            "gold corrections written by crowdworkers; or (2)\n",
            "the next bot replies from the original FITS data\n",
            "that are classiﬁed as good (“self-corrections”). We\n",
            "ﬁne-tuned the reply corrector from a 3B parameter\n",
            "R2C2 transformer model (Shuster et al., 2022a).\n",
            "4.2 Step 2: Predict missing labels\n",
            "In a bot-human dialogue, the binary feedback can\n",
            "be quite sparse, with many replies having no ex-\n",
            "plicit feedback. We thus predict labels for these\n",
            "replies with the satisfaction classiﬁer trained in\n",
            "Step 1a. After this step, every bot reply in the\n",
            "dataset has a binary label either from the original\n",
            "human binary feedback (\n",
            " or\n",
            " ), or predicted\n",
            "by the satisfaction classiﬁer (\n",
            " or\n",
            " ).\n",
            "4.3 Step 3: Convert lemons to cherries\n",
            "We can now augment the training data. We use\n",
            "thereply corrector trained in step 1b to generate\n",
            "improved replies for any examples labeled as bad (\n",
            "or\n",
            " ), and then add them to the training set\n",
            "for the ﬁnal dialogue model.\n",
            "Selecting correctable cases However, we note\n",
            "that not all bad responses are easily correctable\n",
            "given free-form textual feedback. For example,\n",
            "the human feedback “You are talking nonsense!”\n",
            "could help indicate this is a\n",
            " using the satis-\n",
            "faction classiﬁer , but is less helpful for knowing\n",
            "what the right response is, compared to more con-\n",
            "structive feedback such as “That’s a quick topic\n",
            "change! Let’s continue to talk about fruit, perhaps\n",
            "fruit drinks?” We thus experiment with detecting\n",
            "cases that are “correctable”, and only use these\n",
            "to augment our training data. We ﬁrst embed the\n",
            "free-form textual feedback and the immediate next\n",
            "bot reply in recorded conversations with Sentence-\n",
            "BERT (Reimers and Gurevych, 2019), and then\n",
            "calculate their cosine similarity score. If the score\n",
            "is high, it means that the human free-form textual\n",
            "feedback is easier for a model to comprehend and\n",
            "thus revise its own response accordingly. We deﬁne\n",
            "such examples as correctable and then threshold\n",
            "the similarity score to pick out correctable cases.\n",
            "Predicting reply corrections To obtain the cor-\n",
            "rections, we adopt a reranking-based learning\n",
            "method widely used in many previous studies (Nie\n",
            "et al., 2020; Nakano et al., 2021; Askell et al., 2021)\n",
            "to score and rank the generations. We ﬁrst use the\n",
            "reply corrector to generate many correction candi-\n",
            "dates (60 in our experiments). Then we concatenate\n",
            "the original context with the correction candidates\n",
            "and feed them into the satisfaction classiﬁer from\n",
            "Step 1a. Finally, we select the top one with the\n",
            "highest probability output by the classiﬁer as the\n",
            "ﬁnal correction. If all generated corrections are\n",
            "predicted as bad, we will skip this example.\n",
            "4.4 Step 4: Collect fruits and re-train\n",
            "After the previous steps, each bot response is anno-\n",
            "tated with either a gold or predicted binary label,\n",
            "and those labeled as bad are converted from bad\n",
            "responses to good ones using human feedback. The\n",
            "ﬁnal step is to augment the training set of the ﬁnal\n",
            "dialogue model with the new data.\n",
            "One straightforward method to improve the\n",
            "model is to augment the training data with all the\n",
            "positive replies including the corrections (\n",
            " +\n",
            ") and use the standard language modeling objective.\n",
            "However, this standard training does not utilize\n",
            "negative/bad replies (\n",
            " +\n",
            " ) to avoid them. Wehence also apply the recently proposed DIRECTOR\n",
            "model (Arora et al., 2022) to both reinforce the\n",
            "positive responses and penalize the negative ones.\n",
            "DIRECTOR is a uniﬁed decoder-classiﬁer model\n",
            "jointly trained with a language modeling task and\n",
            "a classiﬁcation task. During inference, it uses its\n",
            "language modeling head to predict the next token\n",
            "probability, and its classiﬁer head to decide if the\n",
            "tokens belong to positive examples to generate the\n",
            "ﬁnal output. But it is worth noting that in this step\n",
            "inJUICER , we could use any other approach that\n",
            "utilizes both positive and negative responses to re-\n",
            "train and improve the ﬁnal dialogue model.\n",
            "5 Experimental Setup\n",
            "In our experiments, we used the 3B parameter\n",
            "BlenderBot2 (BB2 3B) (Komeili et al., 2021; Xu\n",
            "et al., 2021) as the base dialogue model and try to\n",
            "improve it with human feedback from deployment.\n",
            "5.1 Datasets: FITS and DEMO\n",
            "We performed experiments on the FITS (Xu et al.,\n",
            "2022) dataset. We also tested the zero-shot trans-\n",
            "ferability of both the satisfaction classiﬁers and\n",
            "thereply correctors on a real deployment dataset\n",
            "DEMO (Ju et al., 2022).\n",
            "5.1.1 FITS\n",
            "FITS contains internet-augmented human-bot dia-\n",
            "logues with annotated feedback for every turn, in-\n",
            "cluding a binary label, free-form textual feedback\n",
            "and a gold response, with around 39k bot utter-\n",
            "ances in total. See Section A.1 for more details. To\n",
            "mimic a deployment setting with limited feedback,\n",
            "we uniformly sampled 20% of the bot responses\n",
            "from the training set of FITS and considered them\n",
            "to have binary feedback and gold labels, while the\n",
            "rest were considered unlabeled. However, we did\n",
            "not remove free-form textual feedback when it is\n",
            "present, as it remains part of the conversation, see\n",
            "Figure 1. Table 4 in the Appendix shows the data\n",
            "statistics after sampling.\n",
            "We used the original FITS validation, test set\n",
            "and unseen test set (of new conversational topics)\n",
            "for evaluation, and employed the same metrics as\n",
            "Xu et al. (2022) for the ﬁnal dialogue models: per-\n",
            "plexity, F1 overlap with the gold annotation, and\n",
            "human evaluation via conversations with the bot.\n",
            "During conversations, crowdworkers click\n",
            " or\n",
            "per turn and give a ﬁnal rating (a score out of\n",
            "5) in the end. We report the average good response\n",
            "rate in percentage.5.1.2 DEMO\n",
            "The dataset DEMO is from the deployment of\n",
            "BlenderBot 3 (Shuster et al., 2022b) with responses\n",
            "veriﬁed by crowdworkers (Ju et al., 2022). In total\n",
            "923 bot responses across 81 conversations are used\n",
            "as an evaluation set.\n",
            "5.2 Baselines\n",
            "We have two categories of baselines: (1) with-\n",
            "out model-augmented data, and (2) with a prompt-\n",
            "based reply corrector . In addition, we also compare\n",
            "with oracle methods using 100% labeled feedback\n",
            "data without sampling.\n",
            "Baselines without augmentation. The most\n",
            "straightforward baselines are to ﬁne-tune with the\n",
            "limited human-labeled feedback only.\n",
            "•Gold corrections from 20% Gold correc-\n",
            "tions provide a strong learning signal. Here,\n",
            "we simply ﬁne-tune BB2 3B on the 20% gold\n",
            "corrections.\n",
            "•Free-form textual feedback from 20% Fol-\n",
            "lowing Hancock et al. (2019), we ﬁne-tune\n",
            "BB2 3B with the context as the input and the\n",
            "free-form textual feedback (identiﬁed as the\n",
            "response following the bad\n",
            " responses) as\n",
            "the target.\n",
            "Baseline with a prompt-based reply corrector .\n",
            "Instead of training a supervised reply corrector\n",
            "with gold corrections, this baseline prompts an off-\n",
            "the-shelf model with free-form textual feedback\n",
            "and instructions like “given the feedback, correct\n",
            "the original response” as a reply corrector to gen-\n",
            "erate corrections, and then ﬁne-tunes the ﬁnal dia-\n",
            "logue model on these corrections.\n",
            "•3B-all-corrections : Scheurer et al. (2022)\n",
            "proposed an approach to improve language\n",
            "models with language feedback, originally ap-\n",
            "plied to summarization tasks, which we adapt\n",
            "here for dialogue. Given a small number (100)\n",
            "human feedback samples, they prompted a\n",
            "language model to condition on the context\n",
            "(input+feedback) to re-generate multiple sum-\n",
            "marization corrections, picked the correc-\n",
            "tion with the highest similarity score with\n",
            "the feedback, and ﬁnally ﬁne-tuned the lan-\n",
            "guage model on the corrections to improve\n",
            "it. In our implementation, we use the base-\n",
            "line BlenderBot 2 model (3B) as the replycorrector . While Scheurer et al. (2022) used\n",
            "larger language models (175B), our implemen-\n",
            "tation of the baseline is more comparable to\n",
            "ourJUICER models since our reply corrector\n",
            "also has 3B parameters. In our experiments,\n",
            "instead of using only 100 examples, we make\n",
            "this a stronger baseline by generating correc-\n",
            "tions for allthe bad replies.\n",
            "5.3 J UICER models\n",
            "We also compare several variants of J UICER .\n",
            "•JUICER . We ﬁne-tune BB2 3B by aug-\n",
            "menting the 20% human-annotated data with\n",
            "(1) predicted good responses by the satisfac-\n",
            "tion classiﬁer from the remaining 80% un-\n",
            "annotated turns, and (2) predicted corrections\n",
            "generated by the reply corrector , ﬁltered to\n",
            "only include the correctable cases rather than\n",
            "using all the predicted corrections.\n",
            "•JUICER +DIRECTOR . We ﬁne-tune using\n",
            "DIRECTOR which uses both the positive and\n",
            "negative replies. Both gold annotations and\n",
            "the ﬁltered corrections generated by the reply\n",
            "corrector are used as positive classiﬁcation\n",
            "data. Bad responses labeled by humans or\n",
            "thesatisfaction classiﬁer are used as negative\n",
            "data for ﬁne-tuning the classiﬁer head.\n",
            "•w/o predicted corrections (from Step 3) . In\n",
            "this ablation, we ﬁne-tune the ﬁnal dialogue\n",
            "model with only predicted good responses by\n",
            "thesatisfaction classiﬁer , without the correc-\n",
            "tions generated by the reply corrector .\n",
            "•w/o selecting correctable cases . In this abla-\n",
            "tion, we only augment with (1) predicted good\n",
            "responses by the satisfaction classiﬁer , and (2)\n",
            "all the predicted corrections without selecting\n",
            "the more correctable cases. This tests if select-\n",
            "ing correctable cases brings improvements.\n",
            "6 Results\n",
            "We ﬁrst evaluate the satisfaction classiﬁer (Ta-\n",
            "ble 1a), and the reply corrector (Table 1b). We\n",
            "then perform both automatic and human evalua-\n",
            "tions on the ﬁnal dialogue models (Table 2 and\n",
            "Table 3).\n",
            "6.1 Satisfaction classiﬁer\n",
            "Table 1a shows the classiﬁers’ performance on the\n",
            "FITS data and also their zero-shot performance on\n",
            "DEMO.Adding the next human response helps. We\n",
            "ﬁnd the balanced accuracy of detecting satisfac-\n",
            "tion using only the dialogue context and the bot\n",
            "response itself is\u001875% on FITS. It is signiﬁcantly\n",
            "improved to\u001895% by including the next human\n",
            "message in the input. A similar improvement is\n",
            "found when measuring balanced F1 as well. On the\n",
            "deployment dataset DEMO where organic users are\n",
            "not required to always write free-form textual feed-\n",
            "back when seeing a bad reply, adding the human\n",
            "response still improves the balanced F1 from 64.77\n",
            "to 71.24, despite this being zero-shot performance\n",
            "(without training on this dataset). These results\n",
            "indicate the importance of using the next human\n",
            "message to make satisfaction classiﬁcation deci-\n",
            "sions. As using the next human response helps, we\n",
            "default to using this satisfaction classiﬁer variant\n",
            "in our standard J UICER setup.\n",
            "6.2 Reply corrector\n",
            "Table 1b shows the results of training the reply cor-\n",
            "rector , comparing different input feature choices.\n",
            "Free-form textual feedback improves the cor-\n",
            "rection. We performed an ablation study where\n",
            "thereply corrector trains on (context + bad reply !\n",
            "good reply ) without the free-form textual feedback,\n",
            "shown in “w/o free-form textual feedback”. As ex-\n",
            "pected, adding free-form textual feedback on what\n",
            "went wrong improves the reply corrector ’s perfor-\n",
            "mance. The best results are relatively close to the\n",
            "oracle performance which uses 100% (rather than\n",
            "20%) gold data for training (23.39 F1 vs. 21.41\n",
            "and 2.93 PPL vs. 3.07).\n",
            "Augmenting with self-correction pairs helps.\n",
            "The standard reply corrector trains on “gold-\n",
            "correction” pairs (context + bad reply + free-form\n",
            "textual feedback!gold correction). Besides these\n",
            "human-written gold corrections, we can also train\n",
            "thereply corrector on “self-correction” pairs (con-\n",
            "text + bad reply + free-form textual feedback !\n",
            "good bot reply ), where a bad reply is followed by\n",
            "a good bot reply either liked by humans or pre-\n",
            "dicted as good by the satisfaction classiﬁer , sug-\n",
            "gesting that the bot “corrects” itself in the follow-\n",
            "ing turn. We found that augmenting with these\n",
            "“self-corrections” improves the F1 from 17.10 to\n",
            "21.41. We can also multitask with various dialogue\n",
            "tasks to further improve the reply corrector ’s per-\n",
            "formance. See Section A.4.1 for more details.(1a) Satisfaction Classiﬁer Valid Test Test Unseen DEMO (zero-shot)\n",
            "Input Acc\" F1\" Acc\" F1\" Acc\" F1\" Acc\" F1\"\n",
            "context+bot+human 94.66 97.25 95.76 97.83 96.74 98.34 59.73 71.24\n",
            "context+bot 75.58 86.07 74.53 85.38 71.46 83.25 56.60 64.77\n",
            "(a)Satisfaction classiﬁer results (classiﬁcation balanced accuracy and balanced f1) on both FITS and DEMO (zero-shot).\n",
            "Adding the next human message helps the satisfaction prediction, even in the zero-shot case.\n",
            "(1b) Reply Corrector Valid Test Test Unseen\n",
            "Input F1\" PPL# F1\" PPL# F1\" PPL#\n",
            "Oracle with 100% annotations\n",
            "gold corrections from 100% 23.39 2.93 21.83 2.63 22.27 4.56\n",
            "w/ free-form textual feedback\n",
            "gold corrections from 20% + self-corrections 21.41 3.07 20.20 2.75 21.77 4.66\n",
            "gold corrections from 20% 17.10 3.37 16.21 2.98 17.91 4.97\n",
            "w/o free-form textual feedback\n",
            "gold corrections from 20% + self-corrections 18.80 3.13 18.36 2.82 18.97 4.84\n",
            "gold corrections from 20% 16.41 3.40 15.08 3.04 16.46 5.06\n",
            "(b)Reply corrector perplexity and F1 on valid/test/test unseen sets. Augmenting with self-corrections improves the result,\n",
            "comparable to the oracle model using 100% gold corrections. Using free-form textual feedback is helpful.\n",
            "Table 1: Performance of the modules in Step 1: (a) satisfaction classiﬁer , and (b) reply corrector .\n",
            "Qualitative results show the corrections make\n",
            "sense. We also include generated correction ex-\n",
            "amples on the FITS data in Appendix Table 8 and\n",
            "on the deployment data in a zero-shot fashion in\n",
            "Appendix Table 9. These examples show that the\n",
            "reply corrector can integrate free-form textual feed-\n",
            "back to correct the bad replies, even for zero-shot\n",
            "deployment data.\n",
            "See section A.4.2 for further details and results\n",
            "on the reply corrector evaluation.\n",
            "6.3 Final dialogue model evaluations\n",
            "The ﬁnal dialogue model results are given in Ta-\n",
            "ble 2 (automatic evaluations) and Table 3 (human\n",
            "evaluations). All methods are ﬁne-tuned from the\n",
            "3B parameter BlenderBot 2 (BB2), making the\n",
            "models comparable.\n",
            "Using J UICER to augment data improves re-\n",
            "sults. JUICER yields signiﬁcant gains over the\n",
            "baseline transformer BB2 3B in both automatic\n",
            "evaluations and human evaluations. For example,\n",
            "we see an F1 increase from 15.3 to 18.5 on the\n",
            "unseen test set, and an improvement of good re-\n",
            "sponses from 33.2% to 41.9% in human evalua-\n",
            "tions. JUICER also performs better than baselines\n",
            "without augmentation (e.g., gold corrections from\n",
            "20%).Our supervised reply corrector outperforms a\n",
            "prompt-based one. Compared to the prompt-\n",
            "based reply corrector baseline (Scheurer et al.,\n",
            "2022), all the JUICER models perform better in\n",
            "automatic evaluations. When the prompt-based\n",
            "model is used as a reply corrector to produce cor-\n",
            "rections to augment the ﬁnal dialogue model train-\n",
            "ing, the ﬁnal model evaluation (F1=14.2, ppl=8.9)\n",
            "is worse than augmenting with the corrections in\n",
            "JUICER (F1=16.7, ppl=8.5).\n",
            "Augmenting training with predicted correc-\n",
            "tions in J UICER helps. JUICER augments train-\n",
            "ing with predicted corrections, which improves\n",
            "both the F1 and perplexity across the board com-\n",
            "pared to JUICER without predicted corrections, e.g.\n",
            "18.5 vs. 17.9 on the test unseen F1. This makes\n",
            "sense because the predicted corrections are gener-\n",
            "ated by the reply corrector given human free-form\n",
            "textual feedback which contains valuable informa-\n",
            "tion, and ﬁne-tuning the ﬁnal dialogue model on\n",
            "these corrections can steer it toward better replies.\n",
            "Selecting correctable cases can help. JUICER\n",
            "picks only correctable cases to augment the training\n",
            "data, with around 62% of cases selected (threshold\n",
            "chosen based on the validation set). Compared to\n",
            "naively augmenting with all predicted corrections,\n",
            "we see gains on valid and unseen test F1 (18.5 vs.Final dialogue model Automatic evaluation\n",
            "Valid Test Test Unseen\n",
            "F1\"PPL#F1\"PPL#F1\"PPL#\n",
            "BB2 3B 14.4 10.6 14.7 10.3 15.3 9.3\n",
            "+gold corrections from 20% 16.2 9.1 15.6 8.9 17.9 8.4\n",
            "+free-form textual feedback from 20% 13.1 10.4 12.6 10.3 13.7 9.6\n",
            "3B-all-corrections (prompt-based) 14.2 8.9 14.5 8.7 15.2 8.2\n",
            "JUICER models\n",
            "+JUICER 16.7 8.5 16.2 8.4 18.5 8.0\n",
            "+JUICER +DIRECTOR 17.2 n/a 16.7 n/a 17.7 n/a\n",
            "JUICER ablations\n",
            "w/o predicted corrections 15.7 9.0 15.8 8.8 17.9 8.2\n",
            "w/o selecting correctable cases 16.4 8.5 16.4 8.4 18.0 8.1\n",
            "Table 2: Final dialogue model automatic evaluation results. All the dialogue models are ﬁne-tuned from BB2 3B.\n",
            "JUICER models with augmentations are better than the baselines without augmentations. J UICER with a supervised\n",
            "reply corrector also performs better than the baseline with a prompted-based reply corrector . DIRECTOR utilizing\n",
            "negative examples is effective. Using predicted corrections and selecting correctable cases are useful.\n",
            "Final dialogue model Human evaluation\n",
            "Good%\"Rating\"\n",
            "BB2 3B 33.2% 3.09\n",
            "+gold corrections from 20% 39.4% 2.89\n",
            "JUICER models\n",
            "+JUICER 41.9% 3.06\n",
            "+JUICER +DIRECTOR 45.5% 3.34\n",
            "Table 3: Final dialogue model human evaluation results.\n",
            "We report the % of good responses and the overall rat-\n",
            "ing, as judged by crowdworkers during conversations.\n",
            "We bold statistically signiﬁcant improvements (inde-\n",
            "pendent two-sample t-test,p < 0:05) of methods over\n",
            "the BB2 3B baseline. J UICER outperforms the base-\n",
            "lines. J UICER +DIRECTOR works the best.\n",
            "18.0), although there is no gain on the seen test set.\n",
            "DIRECTOR provides further gains. DIREC -\n",
            "TOR utilizes both the (predicted) binary feedback\n",
            "signal and textual feedback signal to avoid negative\n",
            "responses. Applying it improves the results fur-\n",
            "ther over standard JUICER (45.5% good responses\n",
            "vs. 41.9% for JUICER without DIRECTOR , as mea-\n",
            "sured by human evaluations). Because DIRECTOR\n",
            "uses a classiﬁer head to decide if a token should\n",
            "be included in the ﬁnal generation, the distribution\n",
            "is altered and perplexity measures are not applica-\n",
            "ble. However, it gives gains in F1 on valid and test\n",
            "sets, although not on the unseen test set. JUICER\n",
            "andDIRECTOR together also outperforms DIREC -\n",
            "TOR alone, even when DIRECTOR uses 100% gold\n",
            "binary labels, see Appendix Table 11. Furthervariants and experiments with DIRECTOR are also\n",
            "given in Section A.5.2.\n",
            "JUICER achieves comparable results to meth-\n",
            "ods with oracle access to gold labels. Com-\n",
            "pared to methods using 100% gold data which\n",
            "was not given to J UICER , our best J UICER models\n",
            "achieve comparable performance, especially on F1\n",
            "and human evaluation. For example, test unseen\n",
            "F1=17.6 for the best “oracle” method vs. 17.7 for\n",
            "the best JUICER model, 47.0% vs. 45.5% good\n",
            "responses, and 3.38 vs. 3.34 in human ratings. See\n",
            "Appendix Table 11 for further details. These “100%\n",
            "data” methods can be seen as upper bound results,\n",
            "showing that JUICER does extract most of the sig-\n",
            "nal from the portion of the dialogue data without\n",
            "binary or gold feedback.\n",
            "See Section A.5 for further experiments and de-\n",
            "tails on ﬁnal model evaluations.\n",
            "7 Conclusion\n",
            "Deployed dialogue agents should continuously im-\n",
            "prove by using human feedback gathered during\n",
            "interactions. Unfortunately, feedback collected in\n",
            "the wild can be limited. In this paper, we proposed\n",
            "JUICER , a framework to efﬁciently use limited or-\n",
            "ganic feedback signals (binary labels and gold cor-\n",
            "rections) if free-form textual feedback is provided.\n",
            "JUICER works by correcting bad responses into\n",
            "good ones to augment the training data for the ﬁ-\n",
            "nal dialogue model. Experiments show that aug-\n",
            "menting with such predictions can integrate human\n",
            "feedback and improve overall performance.8 Limitations and Discussions\n",
            "In our experimental setting, we assume dense free-\n",
            "form textual feedback, i.e., a bad reply is always\n",
            "followed by a free-form message explaining what\n",
            "was wrong. In real deployments, this free-form\n",
            "textual feedback signal may not always be given\n",
            "and without it, the binary satisfaction classiﬁer\n",
            "may not necessarily achieve a high accuracy or\n",
            "F1 (e.g., 90+), which could also impact the later\n",
            "steps. It remains to be seen in real deployments\n",
            "how dense this signal is, and what methods can be\n",
            "used to encourage users to make these signals as\n",
            "dense as possible, so that strong feedback signals\n",
            "are available to train on.\n",
            "We have also assumed good intent from human\n",
            "conversationalists, but it is possible to have adver-\n",
            "sarial and bad actors interacting with the bot. In\n",
            "particular, incorrect feedback or opposite feedback\n",
            "(e.g., thumbs up instead of down) could be supplied\n",
            "by the human for incorrect bot behavior. We see\n",
            "this as an important research direction that should\n",
            "be pursued in parallel to work on algorithms like\n",
            "the ones we study here. See e.g. Ju et al. (2022) for\n",
            "recent work addressing bad actors and adversarial\n",
            "feedback.\n",
            "The training/evaluation loop of JUICER can be\n",
            "long due to its iterative nature. The advantage of\n",
            "using a reply corrector is that we can qualitatively\n",
            "evaluate the quality of the generated corrections.\n",
            "But the drawback is that we need to ﬁrst train a\n",
            "reply corrector , use it to generate corrections, and\n",
            "ﬁnally improve the dialogue. We assume that the\n",
            "best reply corrector will lead to the best ﬁnal dia-\n",
            "logue model, but this remains to be studied. An-\n",
            "other possible direction is to use a latent reply cor-\n",
            "rector to integrate the feedback in a more end-to-\n",
            "end fashion instead of a supervised reply corrector\n",
            "that will generate explicit corrections separate from\n",
            "the dialogue model.\n",
            "Additionally, the proposed JUICER framework\n",
            "improves the dialogue model ofﬂine rather than cor-\n",
            "recting the response on the ﬂy. With the necessary\n",
            "infrastructure support, there is potential for improv-\n",
            "ing the models online. This could be a natural\n",
            "setting for reinforcement learning to get interactive\n",
            "feedback and iteratively update the model policy\n",
            "as the conversation continues. Such a direction\n",
            "does not come without dangers, however, such as\n",
            "the model degrading if it receives poor inputs, e.g.\n",
            "from bad actors as mentioned before.References\n",
            "Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and\n",
            "Jason Weston. 2022. Director: Generator-classiﬁers\n",
            "for supervise language modeling. arXiv preprint\n",
            "arXiv: .\n",
            "Amanda Askell, Yuntao Bai, Anna Chen, Dawn\n",
            "Drain, Deep Ganguli, Tom Henighan, Andy Jones,\n",
            "Nicholas Joseph, Ben Mann, Nova DasSarma, et al.\n",
            "2021. A general language assistant as a laboratory\n",
            "for alignment. arXiv preprint arXiv:2112.00861 .\n",
            "Jason Baumgartner, Savvas Zannettou, Brian Kee-\n",
            "gan, Megan Squire, and Jeremy Blackburn. 2020.\n",
            "The pushshift reddit dataset. arXiv preprint\n",
            "arXiv:2001.08435 .\n",
            "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
            "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
            "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
            "Askell, et al. 2020. Language models are few-shot\n",
            "learners. Advances in neural information processing\n",
            "systems , 33:1877–1901.\n",
            "Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\n",
            "Vishrav Chaudhary, Guillaume Wenzek, Francisco\n",
            "Guzmán, Edouard Grave, Myle Ott, Luke Zettle-\n",
            "moyer, and Veselin Stoyanov. 2019. Unsupervised\n",
            "cross-lingual representation learning at scale. arXiv\n",
            "preprint arXiv:1911.02116 .\n",
            "Ahmed Elgohary, Christopher Meek, Matthew\n",
            "Richardson, Adam Fourney, Gonzalo Ramos, and\n",
            "Ahmed Hassan Awadallah. 2021. Nl-edit: Correct-\n",
            "ing semantic parse errors through natural language\n",
            "interaction. ACL.\n",
            "Braden Hancock, Antoine Bordes, Pierre-Emmanuel\n",
            "Mazare, and Jason Weston. 2019. Learning from\n",
            "dialogue after deployment: Feed yourself, chatbot!\n",
            "arXiv preprint arXiv:1901.05415 .\n",
            "Da Ju, Jing Xu, Y-Lan Boureau, and Jason Weston.\n",
            "2022. Learning from data in the mixed adversarial\n",
            "non-adversarial case: Finding the helpers and ignor-\n",
            "ing the trolls. arXiv preprint arXiv .\n",
            "Mojtaba Komeili, Kurt Shuster, and Jason Weston.\n",
            "2021. Internet-augmented dialogue generation.\n",
            "arXiv preprint arXiv:2107.07566 .\n",
            "Jiwei Li, Alexander H Miller, Sumit Chopra,\n",
            "Marc’Aurelio Ranzato, and Jason Weston. 2016a.\n",
            "Dialogue learning with human-in-the-loop. arXiv\n",
            "preprint arXiv:1611.09823 .\n",
            "Jiwei Li, Alexander H Miller, Sumit Chopra,\n",
            "Marc’Aurelio Ranzato, and Jason Weston. 2016b.\n",
            "Learning through dialogue interactions by asking\n",
            "questions. arXiv preprint arXiv:1612.04936 .\n",
            "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\n",
            "Long Ouyang, Christina Kim, Christopher Hesse,\n",
            "Shantanu Jain, Vineet Kosaraju, William Saunders,et al. 2021. Webgpt: Browser-assisted question-\n",
            "answering with human feedback. arXiv preprint\n",
            "arXiv:2112.09332 .\n",
            "Yixin Nie, Mary Williamson, Mohit Bansal, Douwe\n",
            "Kiela, and Jason Weston. 2020. I like ﬁsh, espe-\n",
            "cially dolphins: Addressing contradictions in dia-\n",
            "logue modeling. arXiv preprint arXiv:2012.13391 .\n",
            "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\n",
            "roll L Wainwright, Pamela Mishkin, Chong Zhang,\n",
            "Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
            "2022. Training language models to follow in-\n",
            "structions with human feedback. arXiv preprint\n",
            "arXiv:2203.02155 .\n",
            "Nils Reimers and Iryna Gurevych. 2019. Sentence-\n",
            "bert: Sentence embeddings using siamese bert-\n",
            "networks. arXiv preprint arXiv:1908.10084 .\n",
            "William Saunders, Catherine Yeh, Jeff Wu, Steven\n",
            "Bills, Long Ouyang, Jonathan Ward, and Jan Leike.\n",
            "2022. Self-critiquing models for assisting human\n",
            "evaluators. arXiv preprint arXiv:2206.05802 .\n",
            "Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan,\n",
            "Angelica Chen, Kyunghyun Cho, and Ethan Perez.\n",
            "2022. Training language models with language feed-\n",
            "back. The First Workshop on Learning with Natural\n",
            "Language Supervision at ACL 2022 .\n",
            "Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\n",
            "Petroni, Patrick Lewis, Gautier Izacard, Qingfei You,\n",
            "Christoforos Nalmpantis, Edouard Grave, and Se-\n",
            "bastian Riedel. 2022. Peer: A collaborative lan-\n",
            "guage model. arXiv preprint arXiv:2208.11663 .\n",
            "Abigail See and Christopher D Manning. 2021. Under-\n",
            "standing and predicting user dissatisfaction in a neu-\n",
            "ral generative chatbot. In Proceedings of the 22nd\n",
            "Annual Meeting of the Special Interest Group on Dis-\n",
            "course and Dialogue , pages 1–12.\n",
            "Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,\n",
            "Stephen Roller, Arthur Szlam, and Jason We-\n",
            "ston. 2022a. Language models that seek for\n",
            "knowledge: Modular search & generation for di-\n",
            "alogue and prompt completion. arXiv preprint\n",
            "arXiv:2203.13224 .\n",
            "Kurt Shuster, Jack Urbanek, Emily Dinan, Arthur\n",
            "Szlam, and Jason Weston. 2020. Deploying life-\n",
            "long open-domain dialogue learning. arXiv preprint\n",
            "arXiv:2008.08076 .\n",
            "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\n",
            "Eric Michael Smith, Stephen Roller, Megan Ung,\n",
            "Moya Chen, Kushal Arora, Joshua Lane, et al.\n",
            "2022b. Blenderbot 3: a deployed conversational\n",
            "agent that continually learns to responsibly engage.\n",
            "arXiv preprint arXiv:2208.03188 .\n",
            "Eric Smith, Mary Williamson, Kurt Shuster, Jason We-\n",
            "ston, and Y-Lan Boureau. 2020. Can you put it all\n",
            "together: Evaluating conversational agents’ ability\n",
            "to blend skills. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-\n",
            "guistics . ACL.\n",
            "Niket Tandon, Aman Madaan, Peter Clark, and Yim-\n",
            "ing Yang. 2022. Learning to repair: Repairing\n",
            "model output errors after deployment using a dy-\n",
            "namic memory of feedback. NAACL Findings.(to\n",
            "appear) .\n",
            "Jing Xu, Arthur Szlam, and Jason Weston. 2021. Be-\n",
            "yond goldﬁsh memory: Long-term open-domain\n",
            "conversation. arXiv preprint arXiv:2107.07567 .\n",
            "Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora,\n",
            "Y-Lan Boureau, and Jason Weston. 2022. Learn-\n",
            "ing new skills after deployment: Improving open-\n",
            "domain internet-driven dialogue with human feed-\n",
            "back. arXiv preprint arXiv:2208.03270 .A Appendix\n",
            "A.1 Details on FITS\n",
            "In this section, we describe the existing FITS task\n",
            "(Xu et al., 2022) in more details. In FITS, each\n",
            "bot message is annotated with the following feed-\n",
            "back. The setting also ensures that after the human\n",
            "provides the feedback, the conversation can be con-\n",
            "tinued with the feedback integrated.\n",
            "• A binary satisfaction label.\n",
            "•If it is a bad reply, the human provides free-\n",
            "form textual feedback on what went wrong in\n",
            "the next human message.\n",
            "•Multiple-choice selection on what the bot\n",
            "could do to improve this turn:\n",
            "(a) using a better search query; or\n",
            "(b) attending better to the search results; or\n",
            "(c) other issues with the overall reply; or\n",
            "(d) no issue (a good reply).\n",
            "•If selecting (a), the human provides a better\n",
            "search query, which will be used in the next\n",
            "turn to continue the conversation.\n",
            "•If selecting (b), the human is presented with\n",
            "the search results and selects the relevant sen-\n",
            "tences, which will be added to the model input\n",
            "in the next turn.\n",
            "•If selecting (c), the human provides a bet-\n",
            "ter overall reply (a gold correction), which\n",
            "is copied to be the next bot response.\n",
            "A.2 Sampling FITS\n",
            "In our experiments, we uniformly sample 20% of\n",
            "the FITS training set to mimic a deployment set-\n",
            "ting with sparse binary and gold feedback. Table 4\n",
            "shows the sampled dataset statistics. Out of the\n",
            "20% of the training set with labels, 1376 exam-\n",
            "ples are “better overall reply” annotated with gold\n",
            "corrections, which accounts for 7% of all bad re-\n",
            "sponses to be corrected in FITS. Those 1376 correc-\n",
            "tions will later be used to train the reply corrector\n",
            "for augmenting limited human feedback. The size\n",
            "of 20% of the FITS training set (7768 examples) is\n",
            "also similar to that of the validation and test sets.Feedback Breakdown Train (20%) Valid Test Test Unseen\n",
            "Total 7768 4245 9726 8907\n",
            "Better Search Query 1056 605 1167 1036\n",
            "Better Results Usage 1383 756 1527 1310\n",
            "Better Overall Reply 1376 714 1493 1372\n",
            "Good Response 3953 2170 5539 5189\n",
            "Table 4: Data statistics of the sampled version of FITS\n",
            "used in our experiments. We sampled 20% from FITS.\n",
            "Note that the training set size of labeled binary feed-\n",
            "back is similar to the test sets.\n",
            "A.2.1 Varying the sampling rate\n",
            "As an ablation study, we varied the sampling rate.\n",
            "Table 5 shows different ﬁnal dialogue models’ re-\n",
            "sults with different sampling rates. The input to the\n",
            "dialogue model is the context, and the output is the\n",
            "human-written gold correction.\n",
            "As we increase the sampling rate, the ﬁnal dia-\n",
            "logue model’s perplexity improves in general, but\n",
            "the gain becomes smaller. For instance, when we\n",
            "only sample 5% from FITS, the validation perplex-\n",
            "ity is 9.52; if we increase the sampling rate to 20%,\n",
            "the perplexity is 9.09; but when we further increase\n",
            "the sampling rate to 30% and 50%, the perplexity\n",
            "becomes 9.12 and 8.80 respectively.\n",
            "We ﬁnd the 20% sampling rate is a good balanc-\n",
            "ing point with both reasonable F1 and perplexity.\n",
            "A.3 Different modules in J UICER\n",
            "There are different modules involved in JUICER\n",
            "and we summarize them in Table 6. To sum up,\n",
            "JUICER has two helper modules, a satisfaction clas-\n",
            "siﬁer and a reply corrector to help improve the ﬁnal\n",
            "dialogue model.\n",
            "Thesatisfaction classiﬁer identiﬁes if the bot re-\n",
            "sponse is satisfactory or not. It is evaluated on both\n",
            "FITS and zero-shot DEMO deployment datasets.\n",
            "Both the reply corrector and the ﬁnal dialogue\n",
            "models are generative models, and are automati-\n",
            "cally evaluated on the human-written gold correc-\n",
            "tions in the FITS validation and test sets, as gold\n",
            "corrections can reﬂect the model’s ability to gener-\n",
            "ate good responses.\n",
            "Thereply corrector converts bad responses into\n",
            "good ones using free-form textual feedback. We\n",
            "evaluate it on gold search results instead of live\n",
            "search results, in order to generate better reply\n",
            "corrections. We describe it in more detail in Sec-\n",
            "tion A.4.\n",
            "The ﬁnal dialogue model is evaluated on live\n",
            "search results from Bing, ﬁltered by Common-Final dialogue model Valid Test Test Unseen\n",
            "Varying the sampling rate F1\"PPL#F1\"PPL#F1\"PPL#\n",
            "Baseline performance varying the sampling rate\n",
            "+gold correction from 20% 16.2 9.1 15.6 8.9 17.9 8.4\n",
            "+gold correction from 5% 16.8 9.5 16.6 9.3 18.9 8.5\n",
            "+gold correction from 10% 16.6 9.3 16.5 9.1 18.9 8.4\n",
            "+gold correction from 30% 15.7 9.1 16.3 8.6 18.1 8.3\n",
            "+gold correction from 50% 16.5 8.8 16.0 9.0 17.9 8.3\n",
            "Table 5: Final dialogue model results varying the sampling rate. Perplexities get better as we increase the sampling\n",
            "rate, but the gain becomes smaller. F1 ﬁrst gets worse and then goes up. These suggest that a sampling rate of 20%\n",
            "is a good balancing point with both a good F1 and a good perplexity.\n",
            "Model Inputs!Outputs Fine-tuned from Evaluated on Description\n",
            "(1a)Satisfaction classiﬁerContext + bot reply (+ the next human\n",
            "response)!binary {good, bad} on the\n",
            "bot reply311M TransformerFITS valid&test\n",
            "and DEMOGiven the context, a bot reply and poten-\n",
            "tially the next human message, detect if\n",
            "the bot reply is satisfactory\n",
            "(1b) Reply correctorContext + bad bot reply + free-form tex-\n",
            "tual feedback!improved reply (“a cor-\n",
            "rection”)3B R2C2 (Shuster\n",
            "et al., 2022a)Gold corrections in\n",
            "FITS valid&test (on\n",
            "gold search results)Given the context, the bad reply, and\n",
            "free-form textual feedback, generate an\n",
            "improved reply (“correct the bad reply”)\n",
            "(2) Final dialogue model Context!reply 3B BlenderBot 2Gold corrections in\n",
            "FITS valid&test (on\n",
            "live search results)Given the context, generate a reply. We\n",
            "ﬁne-tune our models using BlenderBot\n",
            "2 as the base model (Komeili et al.,\n",
            "2021; Xu et al., 2021).\n",
            "Table 6: The input, output, and description of the three models used in J UICER .\n",
            "Crawl, following Xu et al. (2022) instead of gold\n",
            "search results to better reﬂect performance with live\n",
            "users. We describe it in more detail in Section A.5.\n",
            "A.4 Reply corrector\n",
            "Thereply corrector trains on data where for a given\n",
            "example the input consists of the dialogue context\n",
            "+ bad reply to correct + the following human mes-\n",
            "sage, and the output consists of the correction for\n",
            "the bad reply. The models used in the main paper\n",
            "were ﬁne-tuned from the R2C2 transformer (Shus-\n",
            "ter et al., 2022a).\n",
            "A.4.1 Training the reply corrector with\n",
            "multiple tasks\n",
            "In our experiments, we multi-tasked with various\n",
            "dialogue tasks to train the reply corrector , which\n",
            "improves the result. These tasks include the orig-\n",
            "inal reply correction task, the task with context\n",
            "as the input and free-form textual feedback as the\n",
            "target, and the dialogue task of Wizard of Internet\n",
            "(Komeili et al., 2021). We tuned the weights for dif-\n",
            "ferent tasks, and other hyper-parameters (learning\n",
            "rate, batch size, etc) according to the performance\n",
            "on the validation set.\n",
            "A.4.2 Evaluating the reply corrector\n",
            "Since the reply correctors are used to generate cor-\n",
            "rections rather than interacting with live users, weevaluated them with gold search results (which\n",
            "leads to better corrections) instead of live search\n",
            "results from Bing (which better reﬂects the live\n",
            "interaction performance).\n",
            "Although the reply corrector (Table 1b) and the\n",
            "ﬁnal dialogue models (Table 2) are evaluated on\n",
            "the same validation and test subsets that have gold\n",
            "corrections, their results are not comparable be-\n",
            "cause of the following two reasons. First, as men-\n",
            "tioned earlier, the reply correctors condition on\n",
            "the gold search results instead of the live search\n",
            "results, while the ﬁnal dialogue models use the live\n",
            "search results. Second, the reply correctors rely on\n",
            "the free-form textual feedback to convert lemons\n",
            "to cherries, so we also append the free-form tex-\n",
            "tual feedback into the input to the reply correctors ,\n",
            "but for the ﬁnal dialogue model, we do not have\n",
            "the additional free-form textual feedback informa-\n",
            "tion. These are the main reasons why the results in\n",
            "Table 1b are better than those in Table 2.\n",
            "A.4.3 Generating reply corrections\n",
            "We adopt a reranking-based learning method to\n",
            "ﬁrst generate multiple reply corrections, and then\n",
            "use the satisfaction classiﬁer to score and rerank\n",
            "the generated corrections. Because the reply cor-\n",
            "rector ’s performance is good (comparable to the\n",
            "one trained on 100% data in Table 1b) and we gen-(1b) Reply Corrector Valid Test Test Unseen\n",
            "Input F1\" PPL# F1\" PPL# F1\" PPL#\n",
            "ﬁne-tuned from R2C2\n",
            "gold corrections from 20% + self-corrections 21.41 3.07 20.20 2.75 21.77 4.66\n",
            "+ D IRECTOR 22.81 - 22.59 - 22.10 -\n",
            "+ D IRECTOR OVERLAP 23.00 - 22.50 - 22.55 -\n",
            "ﬁne-tuned from BB2\n",
            "gold corrections from 20% + self-corrections 16.32 7.06 14.53 7.01 15.63 7.16\n",
            "Table 7: Reply corrector results. The top block shows the reply corrector ﬁne-tuned from R2C2 with D IRECTOR\n",
            "and D IRECTOR OVERLAP , and the bottom block shows the reply corrector ﬁne-tuned from BB2. R2C2 is better\n",
            "than BB2 as a reply corrector . Using D IRECTOR improves the result. Using D IRECTOR OVERLAP further improves\n",
            "over D IRECTOR .\n",
            "erated 60 correction candidates to choose from,\n",
            "the majority (99.96%, 16893 out of 16989) of bad\n",
            "responses have at least one generated correction\n",
            "that is predicted as satisfactory by the satisfaction\n",
            "classiﬁer .\n",
            "Generated reply correction examples. Table 8\n",
            "and Table 9 show generated reply correction ex-\n",
            "amples on the FITS dataset and the deployment\n",
            "dataset (zero-shot) respectively. These qualitative\n",
            "examples show that the reply corrector can convert\n",
            "bad replies into good ones using free-form textual\n",
            "feedback, even for unseen deployment data.\n",
            "A.4.4 Using BB2 to train the reply corrector\n",
            "The models used in the main experiments were ﬁne-\n",
            "tuned from R2C2 (Shuster et al., 2022a). We also\n",
            "report results ﬁne-tuned with BB2 in Table 7. We\n",
            "ﬁnd that BB2 is worse than R2C2 as a reply cor-\n",
            "rector because its generated corrections are more\n",
            "like conversational replies rather than actual cor-\n",
            "rections.\n",
            "A.4.5 Using D IRECTOR in the reply corrector\n",
            "Using DIRECTOR to combine multiple feedback\n",
            "signals is also effective for the reply corrector . We\n",
            "can use DIRECTOR to further improve the reply cor-\n",
            "rector ’s F1 to 22.81, as shown in Table 7, where the\n",
            "positive examples are the gold corrections and the\n",
            "negative examples are the bad bot responses. How-\n",
            "ever, although the F1 of the DIRECTOR -enhanced\n",
            "reply corrector is better, we ﬁnd that if we use it\n",
            "to generate reply corrections to improve the ﬁnal\n",
            "dialogue models, the F1 is slightly better but the\n",
            "perplexity gets worse than using a regular reply cor-\n",
            "rector without DIRECTOR , as shown in Table 10.\n",
            "More analysis is needed to understand the reasons\n",
            "for this.A.5 Final dialogue model evaluation\n",
            "We evaluate the ﬁnal dialogue model on live search\n",
            "results instead of gold search results to better reﬂect\n",
            "performance with live users.\n",
            "A.5.1 Oracle performance using 100%\n",
            "feedback data\n",
            "Xu et al. (2022) trained various methods on the en-\n",
            "tire FITS dataset. Since our method is trained only\n",
            "on 20% of FITS, the 100% models’ performance\n",
            "could be viewed as an upper bound of our models.\n",
            "They also used the 3B parameter BlenderBot 2 as\n",
            "a base model for the ﬁnal dialogue model, making\n",
            "it comparable to our experiments. Their results are\n",
            "in Table 11 and we detail their models below.\n",
            "•100% gold correction . The input is the con-\n",
            "text and the target is the gold correction (6,601\n",
            "in the entire FITS dataset). This can be di-\n",
            "rectly compared to “gold correction from 20%”\n",
            "in Table 2.\n",
            "•100% free-form textual feedback . The in-\n",
            "put is the context and the target is the free-\n",
            "form textual feedback. This should be com-\n",
            "pared to “free-form textual feedback from\n",
            "20%” in Table 2.\n",
            "•100% module supervision . BlenderBot 2\n",
            "is an internet-augmented bot with different\n",
            "modules such as a search module to gener-\n",
            "ate a search query, and a knowledge mod-\n",
            "ule to attend to the search results. Using\n",
            "the human-written gold search query, human-\n",
            "selected search doc and gold correction, they\n",
            "ﬁne-tuned each individual module to improve\n",
            "BlenderBot 2.From Utterance\n",
            "USRHi bot, what supplies do I need for a\n",
            "diaper change?\n",
            "BOTThat’s a good question. I’m not sure,\n",
            "but I’m sure you can ﬁnd out online.\n",
            "FeedbackThe results list at least one necessary\n",
            "supply, with the implication that bot\n",
            "would ﬁnd others if it read further.\n",
            "CorrectionBring your supplies to the diapering\n",
            "area. Clean cloth diapers are essential.\n",
            "You’ll also need a clean diaper, clean\n",
            "wipes, diaper cream, gloves, plastic or\n",
            "waterproof bag for soiled clothing, extra\n",
            "clothes.\n",
            "OriginalIt depends on how many diapers you\n",
            "have. If you have a lot of diapers, you\n",
            "will need a changing table and a chang-\n",
            "ing pad.From Utterance\n",
            "USR What is the name of hello kitty’s sister?\n",
            "BOT Hello kitty’s twin sister, mimmy.\n",
            "FeedbackThat’s great. I think to make this even\n",
            "better it could be phrased a little more\n",
            "conversationally. Something like \"hello\n",
            "kitty has a twin sister, mimmy.\"\n",
            "CorrectionHello kittys sister is called mimmy! I\n",
            "think it’s funny she looks a lot like her.\n",
            "Do you like hello kitties sister?\n",
            "OriginalHello kitty has a twin sister named\n",
            "mimmy.\n",
            "From Utterance\n",
            "USRCan you tell me what two sides fought\n",
            "during the war of the roses?\n",
            "BOTThe war of the roses was between the\n",
            "english and the french. The english\n",
            "won.\n",
            "FeedbackIt looks like that answer is not right. The\n",
            "search results say something different.\n",
            "CorrectionThe war was fought between two com-\n",
            "peting royal families: the yorks and the\n",
            "lancasters, both members of the age-old\n",
            "royal plantagenet family.\n",
            "Original The english and the french.From Utterance\n",
            "USR What are risk factors of asthma?\n",
            "BOTAsthma is a chronic disease of the air\n",
            "passages in the lungs.\n",
            "Feedback Try using a more speciﬁc query\n",
            "CorrectionAsthma can be caused by a combina-\n",
            "tion of genetic and environmental fac-\n",
            "tors. These factors include: early expo-\n",
            "sure to allergens, changing in weather,\n",
            "respiratory infections and more.\n",
            "OriginalAsthma is a chronic disease of the air\n",
            "passages in the lungs.\n",
            "Table 8: Qualitative correction examples generated by the best reply corrector on FITS.\n",
            "•100% reward-based learning . They also\n",
            "adopted a reward-based learning approach,\n",
            "and built a satisfaction classiﬁer to identify\n",
            "good and bad responses. They used the vanilla\n",
            "BlenderBot 2 model to generate multiple re-\n",
            "sponses, then reranked them with the score\n",
            "from the classiﬁer as the reward, and chose\n",
            "the response with the highest reward. Finally,\n",
            "they ﬁne-tuned BlenderBot 2 on the responses\n",
            "with high rewards to improve it.\n",
            "•100% D IRECTOR . They also used both the\n",
            "binary satisfaction labels and the textual feed-\n",
            "back to train DIRECTOR models to further\n",
            "improve the performance.\n",
            "As mentioned in the main body of our paper,\n",
            "JUICER achieves comparable performance to the“oracle” (100%) models in F1 and human evalua-\n",
            "tion. For instance, the best oracle method which\n",
            "ﬁne-tunes individual modules achieves an F1 of\n",
            "17.2 and 47.0% good response rate, and a hu-\n",
            "man rating of 3.38, while the best JUICER model\n",
            "achieves an F1 of 17.2, a good response rate of\n",
            "45.5%, and a similar human rating of 3.34.\n",
            "A.5.2 DIRECTOR OVERLAP\n",
            "We also develop a new variant of DIRECTOR and\n",
            "use it to improve the ﬁnal dialogue model. In DI-\n",
            "RECTOR , every token in the positive and negative\n",
            "examples has a one or zero label respectively. In\n",
            "our setting, we have a pair of a bad response and\n",
            "a good response (a gold correction), e.g., “I like\n",
            "watermelons too! Have you heard of Harry Styles?”\n",
            "(bad) and “I like watermelons too! They tastesFrom Utterance\n",
            "Truncated ...\n",
            "BOTHello! How are you today? I’m doing\n",
            "well, thank you. What are you up to?\n",
            "USR I didn’t ask how you are doing?\n",
            "BOTOh, I’m sorry. I meant to say, how are\n",
            "you and your family doing today?\n",
            "USR I don’t have a family\n",
            "BOTI’m sorry to hear that. Are you a mem-\n",
            "ber of the family motor coach associa-\n",
            "tion (fmca)?\n",
            "FeedbackSomething like: “i’m sorry I assumed\n",
            "you had a family! I’ll keep this in mind\n",
            "going forward”\n",
            "Correction 1You’re right. I shouldn’t have assumed\n",
            "you have a family. I will keep this is\n",
            "mind going foward.\n",
            "Correction 2I apologize. I didn’t mean to assume\n",
            "you had children! I hope you have a\n",
            "great day.From Utterance\n",
            "Truncated ...\n",
            "BOTWhich operators do you like the most?\n",
            "I like the operators from years 1 and 5\n",
            "of siege.\n",
            "USR I’m not sure, as I’ve never played.\n",
            "BOTHave you played any of the other rain-\n",
            "bow six games? I’ve played rainbow six\n",
            "3 and 4.\n",
            "USR No.\n",
            "BOTNo, I haven’t. I’ve only played the ﬁrst\n",
            "rainbow six game. Have you played that\n",
            "one?\n",
            "FeedbackYou could suggest other games, or ex-\n",
            "plain what rainbow six is about.\n",
            "Correction 1Rainbow siege is a ﬁrst-person tactical\n",
            "shooter. It is a good game. Do you like\n",
            "it?\n",
            "Correction 2Do you like any other games? What\n",
            "console do you play on? Do you game\n",
            "on pc or console?\n",
            "From Utterance\n",
            "Truncated ...\n",
            "USR Try saying good night, or sweet dreams.\n",
            "BOTThanks for your feedback. It is good to\n",
            "know.\n",
            "USRAlright, I’m going to bed now. Good\n",
            "night!\n",
            "BOTThanks! I’ve been trying to get into sci\n",
            "ﬁ, but it’s hard to ﬁnd a good book.\n",
            "FeedbackYou could say good night, or something\n",
            "similar.\n",
            "Correction 1Well, you should go to bed so you can\n",
            "sleep well. Good night.\n",
            "Correction 2 Have a good night!From Utterance\n",
            "BOTDo you know how someone can make\n",
            "the best pizza?\n",
            "USR I don’t, tell me?\n",
            "BOTThere is an international pizza expo in\n",
            "las vegas that showcases top suppliers\n",
            "and education about pizza.\n",
            "FeedbackThat does not tell me about how to\n",
            "make the best pizza.\n",
            "Correction 1The best pizza is made from fresh ingre-\n",
            "dients with high quality toppings and is\n",
            "cooked to perfection. The best sauce is\n",
            "fresh basil.\n",
            "Correction 2Many pizza shops have training classes\n",
            "to help you learn how to prepare and\n",
            "make pizza. There are also books to\n",
            "help.\n",
            "Table 9: Zero-shot corrections generated by the best reply corrector on unseen deployment data.\n",
            "great in drinks.” (good). Since people tend to edit\n",
            "the original bad response to correct it, they may\n",
            "have many overlapping tokens (“I like watermel-\n",
            "ons too!”), which we do not have to punish. So we\n",
            "develop DIRECTOR OVERLAP , where we obtain\n",
            "the bag of tokens of the pair of the bad response\n",
            "and the gold correction, and assign a positive label\n",
            "for the overlapping tokens in the negative examples.\n",
            "In our data, 28.4% of tokens in the bad responses\n",
            "overlap with those in gold corrections (6.5% are\n",
            "stop words and punctuations, and 21.9% are not).Table 7 and Table 12 show the result of DIREC -\n",
            "TOR OVERLAP . For the reply corrector ,DIRECTOR\n",
            "OVERLAP improves the F1 to 23.00 over DIREC -\n",
            "TOR. For the ﬁnal dialogue model, DIRECTOR\n",
            "OVERLAP improves the good response rate and\n",
            "lowers the search result error in human evaluations\n",
            "over D IRECTOR .\n",
            "B Model Training Setting\n",
            "We use the openly available ParlAI framework for\n",
            "all training runs, as well as for evaluations, whereFinal dialogue model Automatic evaluation\n",
            "Valid\n",
            "F1\" PPL#\n",
            "JUICER models\n",
            "+JUICER 16.7 8.5\n",
            "+JUICER w/ D IRECTOR OVERLAP -based reply corrector 16.8 8.8\n",
            "JUICER ablations\n",
            "w/o selecting correctable cases 16.4 8.5\n",
            "w/o selecting correctable cases w/ D IRECTOR OVERLAP -based reply corrector 16.5 8.7\n",
            "Table 10: Final dialogue model automatic evaluation results. The D IRECTOR OVERLAP -enhanced reply corrector\n",
            "achieves the highest F1 on the reply correction task, better than the regular reply corrector (see Table 7). But when\n",
            "we use it to generate the reply corrections to further improve the ﬁnal dialogue model, we can improve the F1 of the\n",
            "ﬁnal dialogue model slightly, but the perplexity gets a bit worse. Further investigations are needed to understand\n",
            "the reason for this.\n",
            "Oracle model performance Automatic evaluation Human evaluation\n",
            "Error Breakdown #\n",
            "Valid Test Test Unseen GoodRating\"Search SearchResponseF1\"PPL#F1\"PPL#F1\"PPL#response\" Query Results\n",
            "BB2 14.4 10.6 14.7 10.3 15.3 9.3 33.2% 3.09 12.1% 18.6% 18.1%\n",
            "+100% reward-based learning 15.1 11.0 14.2 10.7 14.3 9.6 36.4% 2.83 11.3% 18.6% 17.0%\n",
            "+100% free-form textual feedback 15.5 9.7 15.6 9.5 16.8 8.7 37.0% 3.22 11.6% 17.6% 17.0%\n",
            "+100% gold correction 14.7 8.2 15.5 8.0 17.0 8.0 40.3% 3.37 11.6% 18.3% 15.0%\n",
            "+100% module supervision 14.9 7.6 15.5 7.5 15.4 8.3 42.0% 3.35 8.4% 20.8% 14.4%\n",
            "+100% reranking binary feedback 15.8 n/a 15.8 n/a 16.3 n/a - - - - -\n",
            "+100% D IRECTOR binary feedback only 16.2 n/a 16.2 n/a 17.6 n/a 37.8% 3.07 11.4% 17.3% 16.9%\n",
            "+100% D IRECTOR module+binary feedback 17.2 n/a 16.6 n/a 16.0 n/a 47.0% 3.38 8.4% 16.1% 14.3%\n",
            "Table 11: Final dialogue model results from 100% oracle methods in Xu et al. (2022). Similarly we bold statisti-\n",
            "cally signiﬁcant improvements (independent two-sample t-test,p< 0:05) of methods over their baselines BB2 3B\n",
            "in the human evaluation block.\n",
            "Final dialogue model Automatic evaluation Human evaluation\n",
            "Error Breakdown #\n",
            "Valid Test Test Unseen GoodRating\"Search SearchResponseF1\" PPL# F1\" PPL# F1\" PPL#response\" Query Results\n",
            "JUICER\n",
            "+JUICER 16.74 8.50 16.18 8.44 18.50 8.02 41.9% 3.06 13.0% 17.7% 13.8%\n",
            "+JUICER + D IRECTOR 17.25 - 16.70 - 17.70 - 45.5% 3.34 11.3% 17.4% 12.9%\n",
            "+JUICER + D IRECTOR OVERLAP 17.32 - 16.66 - 17.62 - 47.8% 3.25 11.0% 14.8% 13.3%\n",
            "JUICER w/o selecting correctable cases\n",
            "+JUICER 16.44 8.54 16.37 8.41 17.95 8.12 41.4% 3.08 13.4% 16.8% 14.2%\n",
            "+JUICER + D IRECTOR 17.23 - 16.62 - 17.93 - 44.6% 3.40 11.6% 16.7% 13.6%\n",
            "+JUICER + D IRECTOR OVERLAP 16.98 - 16.56 - 17.19 - 45.5% 3.48 10.8% 15.2% 14.3%\n",
            "Table 12: J UICER with D IRECTOR OVERLAP . DIRECTOR OVERLAP improves the human evaluation results over\n",
            "the vanilla D IRECTOR . Similarly we bold statistically signiﬁcant improvements (independent two-sample t-test,\n",
            "p< 0:05) of methods over their baselines BB2 3B in the human evaluation block.\n",
            "metrics are measured using default settings. All\n",
            "the ﬁne-tuned models are trained with a maximum\n",
            "of eight 32GB GPUs (NVIDIA V100), optimized\n",
            "with Adam using \f1= 0:9,\f2= 0:999,\u000f=\n",
            "1e\u000008. Models are trained up to 4000 updates\n",
            "with batch sizes up to 128. The typical ﬁne-tuning\n",
            "time for a standard transformer encoder-decoder is8 hrs before it early stops, and the time is 16 hrs\n",
            "for retrieval-based models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = split_text(text)\n",
        "print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EJeaI2Ybgsv",
        "outputId": "cb072722-d6c2-4f9f-dd49-15912cf6da04"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When Life Gives You Lemons\\n , Make Cherryade\\n :\\nConverting Feedback from Bad Responses into Good Labels\\nWeiyan Shiy\\nMeta AI & Columbia UniversityEmily Dinan\\nMeta AIKurt Shuster\\nMeta AI\\nJason Weston\\x03\\nMeta AIJing Xu\\x03\\nMeta AI\\nAbstract\\nDeployed dialogue agents have the potential\\nto integrate human feedback to continuously\\nimprove themselves. However, humans may\\nnot always provide explicit signals when the\\nchatbot makes mistakes during interactions. In\\nthis work, we propose J UICER , a framework to\\nmake use of both binary and free-form textual\\nhuman feedback. It works by: (i) extending\\nsparse binary feedback by training a satisfac-\\ntion classiﬁer to label the unlabeled data; and\\n(ii) training a reply corrector to map the bad\\nreplies to good ones. We ﬁnd that augment-\\ning training with model-corrected replies im-\\nproves the ﬁnal dialogue model, and we can\\nfurther improve performance by using both\\npositive and negative replies through the re-\\ncently proposed D IRECTOR model.\\n1 Introduction', 'proves the ﬁnal dialogue model, and we can\\nfurther improve performance by using both\\npositive and negative replies through the re-\\ncently proposed D IRECTOR model.\\n1 Introduction\\nExisting dialogue models are primarily trained on\\nhuman-human conversations (Conneau et al., 2019;\\nBaumgartner et al., 2020; Smith et al., 2020). As\\ndialogue agents become increasingly powerful and\\ncarry substantial conversations with humans (Shus-\\nter et al., 2022b), it becomes pressing to have the\\nmodels learn from dialogue successes and failures\\nin the wild, and hence improve after deployment.\\nPrior work has studied how to collect and learn\\nfrom feedback in human-model dialogues (Li et al.,\\n2016a,b; Hancock et al., 2019; Xu et al., 2022). But\\nmost existing methods were proposed under set-\\ntings where either feedback can be obtained when-\\never needed or all turns are annotated with human\\nfeedback. For instance, Xu et al. (2022) introduced\\na dataset with all turns annotated by crowdworkers', 'ever needed or all turns are annotated with human\\nfeedback. For instance, Xu et al. (2022) introduced\\na dataset with all turns annotated by crowdworkers\\nwith three types of feedback: (1) binary thumbs\\nup/down; (2) free-form textual feedback on what\\nwent wrong; (3) gold corrections on what the bot\\nshould have said instead. Unfortunately, annota-\\nyWork done when interning at Meta AI.\\n* Equal contribution.tions such as thumb ups/downs and gold correc-\\ntions are often sparse in real-life deployment set-\\ntings. For example, human conversationalists give\\nthumbs up/down to bot messages in conversations\\nwith the deployed BlenderBot3 model around 5-6%\\nof the time (Shuster et al., 2022b). On the other\\nhand, human conversationalists may express their\\ndissatisfaction with bad responses and explain what\\nwent wrong more naturally in free-form textual\\nfeedback as part of the conversation, rather than\\nproviding the exact gold corrections to those bot', 'dissatisfaction with bad responses and explain what\\nwent wrong more naturally in free-form textual\\nfeedback as part of the conversation, rather than\\nproviding the exact gold corrections to those bot\\nresponses. Therefore, in this paper we study how to\\nutilize sparse binary and gold correction feedback,\\nand relatively dense free-form textual feedback to\\nimprove dialogue models during deployment.\\nIn this work, we introduce JUICER , a framework\\nto “squeeze the juice” out of the sparse human feed-\\nback in human-model conversations to improve the\\ndialogue models after deployment. JUICER con-\\nsists of four steps: (1) we ﬁrst train a binary satis-\\nfaction classiﬁer and a reply corrector on existing\\nbinary feedback and gold corrections; (2) we then\\nuse the satisfaction classiﬁer to label all the bot re-\\nsponses that are missing human labels; (3) next we\\nuse the reply corrector to correct bad bot responses\\n(lemons\\n ) into good ones, conditioning on hu-', 'sponses that are missing human labels; (3) next we\\nuse the reply corrector to correct bad bot responses\\n(lemons\\n ) into good ones, conditioning on hu-\\nman textual feedback; (4) ﬁnally we augment the\\ntraining data with the new good responses (cher-\\nryade\\n ) and re-train our ﬁnal dialogue models.\\nTo evaluate JUICER on state-of-the-art chatbots\\nin such a setting, we thus construct a new sparse\\nsampled version of the existing FITS dataset from\\nXu et al. (2022), which consists of fully annotated\\nhuman-model conversations between users and ex-\\nisting state-of-the-art internet-augmented models\\nsuch as BlenderBot 2 (Komeili et al., 2021; Xu\\net al., 2021) and SeeKeR (Shuster et al., 2022a).\\nWe explore a variety of methods to take advan-\\ntage of limited human feedback at each step of the\\nJUICER framework. Our main results are:arXiv:2210.15893v1  [cs.CL]  28 Oct 2022•We show that free-form textual feedback is\\na very useful signal for improving the per-', 'JUICER framework. Our main results are:arXiv:2210.15893v1  [cs.CL]  28 Oct 2022•We show that free-form textual feedback is\\na very useful signal for improving the per-\\nformance of both a satisfaction classiﬁer to\\nidentify good and bad responses, and a reply\\ncorrector to generate better corrections.\\n•Augmenting training data with reply-\\ncorrector -generated corrections works\\nbetter than only training with existing gold\\ncorrections.\\n•Models such as DIRECTOR (Arora et al.,\\n2022) that utilize both gold/predicted good\\nand bad responses further improves the ﬁnal\\ndialogue model. Our ﬁnal best models out-\\nperform the baseline BlenderBot 2 model or\\nusing D IRECTOR alone.\\n2 Related Work\\nMany recent works have studied how to align lan-\\nguage models with human feedback (Nakano et al.,\\n2021; Ouyang et al., 2022; Scheurer et al., 2022;\\nSaunders et al., 2022; Schick et al., 2022). For in-\\nstance, InstructGPT (Ouyang et al., 2022) was ﬁne-\\ntuned using feedback from labelers who ranked', '2021; Ouyang et al., 2022; Scheurer et al., 2022;\\nSaunders et al., 2022; Schick et al., 2022). For in-\\nstance, InstructGPT (Ouyang et al., 2022) was ﬁne-\\ntuned using feedback from labelers who ranked\\nmodel outputs. Scheurer et al. (2022) ﬁne-tuned\\nGPT-3 and InstructGPT on 100 examples of free-\\nform textual feedback from humans to improve\\nsummarization tasks and found that only the larger\\nmodels such as GPT-3 (175B) (Brown et al., 2020)\\ncan generate accurate reﬁnements using feedback.\\nSaunders et al. (2022) ﬁne-tuned large language\\nmodels to generate self-critiques for summarization\\ntasks to assist human annotators, and continued to\\nreﬁne the models on feedback. In this work, we\\nfocus on improving dialogue agents given various\\nhuman feedback signals (binary, free-form natural\\nlanguage and gold corrections) and compare our\\nmethods to Scheurer et al. (2022).\\nExisting works have also studied how to correct\\nlanguage model output. For instance, Elgohary', 'language and gold corrections) and compare our\\nmethods to Scheurer et al. (2022).\\nExisting works have also studied how to correct\\nlanguage model output. For instance, Elgohary\\net al. (2021) proposed a model to understand natu-\\nral language feedback and produce a series of edits\\nto correct a text-to-SQL semantic parser. Tandon\\net al. (2022) trained a memory-augmented corrector\\nto convert feedback to edits and ﬁx model outputs\\nfor a script generation task. Some recent large lan-\\nguage model research can also repair generations\\ngiven human feedback (Scheurer et al., 2022; Saun-\\nders et al., 2022).\\nPast research has also explored how to integrate\\nfeedback into dialogue agents (Li et al., 2016a,b;Hancock et al., 2019; Shuster et al., 2020; Xu et al.,\\n2022). Li et al. (2016a) investigated how to im-\\nprove the chatbot’s question-answering ability with\\ngeneral textual feedback in a reinforcement learn-\\ning setting. Hancock et al. (2019) developed a', '2022). Li et al. (2016a) investigated how to im-\\nprove the chatbot’s question-answering ability with\\ngeneral textual feedback in a reinforcement learn-\\ning setting. Hancock et al. (2019) developed a\\nself-feeding chatbot that can construct new exam-\\nples from existing human-bot conversations and\\nask for feedback when necessary to improve itself.\\nXu et al. (2022) proposed a dataset with internet-\\naugmented dialogues, where each turn is annotated\\nwith human feedback, and they found that continu-\\nously retraining the model on binary feedback after\\ndeployment is helpful. Our work focuses on con-\\nverting bad responses into good ones to augment\\nthe data and learn from feedback about failures.\\n3 Human Feedback Setting\\nAs illustrated in the dialogue example in Figure 1,\\nwe consider a deployed system where one can col-\\nlect three types of feedback:\\n(1)binary feedback , where the human conversa-\\ntionalist explicitly likes (\\n ) or dislikes (\\n) a bot response;', 'we consider a deployed system where one can col-\\nlect three types of feedback:\\n(1)binary feedback , where the human conversa-\\ntionalist explicitly likes (\\n ) or dislikes (\\n) a bot response;\\n(2)free-form textual feedback , where the hu-\\nman explains conversationally what was\\nwrong when they dislike a response (e.g.,\\n“That’s a quick topic change! Let’s continue to\\ntalk about fruit, perhaps fruit drinks?” );\\n(3)gold correction , where the human conversa-\\ntionalist suggests an alternative reply the bot\\nshould have said, (e.g., “I like watermelons\\ntoo! They tastes great in drinks. ” ).\\nIn a deployment setting, it is unnatural to ask users\\nto always click the thumbs up and down and pro-\\nvide gold corrections whenever the bot makes a\\nmistake. Instead, users tend to provide free-form\\ntextual feedback on what was wrong in their dia-\\nlogue response to express dissatisfaction when the\\nbot makes errors (See and Manning, 2021). There-\\nfore many responses may be missing binary feed-', 'textual feedback on what was wrong in their dia-\\nlogue response to express dissatisfaction when the\\nbot makes errors (See and Manning, 2021). There-\\nfore many responses may be missing binary feed-\\nback (Shuster et al., 2022b). In this paper, we con-\\nsider a sparse thumbs up/down signal and sparse\\ngold correction signal setting, but a dense free-form\\ntextual feedback signal (i.e., mistakes are followed\\nby textual feedback). After collecting conversa-\\ntions with these feedback signals, we can consider\\nmethods to utilize them to improve the dialogue\\nmodel.Figure 1: Our J UICER model. During deployment, we collect three types of human feedback: (1) binary thumbs up\\nand down; (2) free-form textual feedback on what was wrong with the reply ( “That’s a quick topic change! Let’s\\ncontinue to talk about fruit, perhaps fruit drinks?” ); (3) gold corrections of poor replies ( “I like watermelons too!', 'continue to talk about fruit, perhaps fruit drinks?” ); (3) gold corrections of poor replies ( “I like watermelons too!\\nThey tastes great in drinks. ” ). In J UICER , (1) we ﬁrst train a satisfaction classiﬁer and a reply corrector on existing\\nfeedback, (2) we then use the satisfaction classiﬁer to predict binary satisfaction labels for the un-annotated turns,\\n(3) next we use the reply corrector to convert the bad replies to good ones, (4) ﬁnally we collect the good and bad\\nreplies including corrections and re-train the ﬁnal dialogue model to improve it with human feedback.\\n4 The JUICER Method\\nFigure 1 shows the overview of our framework\\nJUICER to incorporate limited human feedback to\\nimprove the deployed dialogue model. The frame-\\nwork consists of training a satisfaction classiﬁer , a\\nreply corrector , and eventually the ﬁnal dialogue\\nmodel itself. We deﬁne the notation here. For a\\ngiven bot reply: (1)\\n denotes un-annotated turns;\\n(2)\\n and\\n : annotated as good or bad responses', 'reply corrector , and eventually the ﬁnal dialogue\\nmodel itself. We deﬁne the notation here. For a\\ngiven bot reply: (1)\\n denotes un-annotated turns;\\n(2)\\n and\\n : annotated as good or bad responses\\nby users, as deﬁned before; (3)\\n : predicted as\\nbad by the satisfaction classiﬁer ; (4)\\n : predicted\\nas good by the satisfaction classiﬁer .\\nJUICER involves four steps, summarized here:\\n1.Step 1. Train two supervised models: a sat-\\nisfaction classiﬁer to detect good and bad\\nreplies, and a reply corrector to correct the\\nbad replies to good ones.\\n2.Step 2. Apply the satisfaction classiﬁer to\\npredict binary labels for all replies missingbinary feedback (\\n !\\n or\\n ). After\\nthis step, each bot reply has a label.\\n3.Step 3. Use the reply corrector to convert the\\nbad replies (those that are either disliked by\\nhuman users or are predicted as bad by the\\nsatisfaction classiﬁer inStep 2 to good replies\\n(\\n!\\n ,\\n!\\n ).\\n4.Step 4. Re-train the ﬁnal dialogue model by\\naugmenting the training data with the good\\n(', 'human users or are predicted as bad by the\\nsatisfaction classiﬁer inStep 2 to good replies\\n(\\n!\\n ,\\n!\\n ).\\n4.Step 4. Re-train the ﬁnal dialogue model by\\naugmenting the training data with the good\\n(\\n +\\n ) and bad (\\n +\\n ) replies de-\\nrived from human feedback and the predic-\\ntions from the previous steps.\\nNow we describe each step in more detail.\\n4.1 Step 1: Train satisfaction classiﬁer and\\nreply corrector on existing feedback\\nWe ﬁrst train two models: (1) a satisfaction classi-\\nﬁer, and (2) a reply corrector in order to build anaugmented training set in later steps. In our experi-\\nments, both models are trained with human-labeled\\ndata which come from the FITS task (Xu et al.,\\n2022), described further in Section 5.1.1.\\n(1a) Satisfaction classiﬁer The training target of\\nthesatisfaction classiﬁer is a binary satisfaction\\nlabel (\\n or\\n ). For the input to the classiﬁer,\\nwe experimented with two variants: (1) the context\\n+ the bot reply to be labeled, and (2) the context', 'label (\\n or\\n ). For the input to the classiﬁer,\\nwe experimented with two variants: (1) the context\\n+ the bot reply to be labeled, and (2) the context\\n+ the bot reply to be labeled + the next human re-\\nsponse. As shown in the example in Figure 1, when\\nthe ﬁrst bot reply is given a thumbs-up, the next\\nhuman response is a natural continuation of the con-\\nversation (e.g., “I also like watermelons” ); when\\nthe bot reply is disliked (the second bot reply), the\\nnext human response is free-form textual feedback\\non what went wrong (e.g., “That’s a quick topic\\nchange! Let’s continue to talk about fruit, perhaps\\nfruit drinks?” ). Hence, the next human response\\ncan be indicative of the quality of its preceding\\nbot reply, and we include it in the input. In our\\nexperiments, the satisfaction classiﬁer is trained\\nby ﬁne-tuning a 311M-parameter transformer pre-\\ntrained on pushshift.io Reddit data (Baumgartner\\net al., 2020).\\n(1b) Reply corrector The input to the reply cor-', 'by ﬁne-tuning a 311M-parameter transformer pre-\\ntrained on pushshift.io Reddit data (Baumgartner\\net al., 2020).\\n(1b) Reply corrector The input to the reply cor-\\nrector is the context + the bad bot reply to correct\\n+ the next human free-form textual feedback on\\nwhat went wrong. The training target is the cor-\\nrection to the bad reply which can be either (1)\\ngold corrections written by crowdworkers; or (2)\\nthe next bot replies from the original FITS data\\nthat are classiﬁed as good (“self-corrections”). We\\nﬁne-tuned the reply corrector from a 3B parameter\\nR2C2 transformer model (Shuster et al., 2022a).\\n4.2 Step 2: Predict missing labels\\nIn a bot-human dialogue, the binary feedback can\\nbe quite sparse, with many replies having no ex-\\nplicit feedback. We thus predict labels for these\\nreplies with the satisfaction classiﬁer trained in\\nStep 1a. After this step, every bot reply in the\\ndataset has a binary label either from the original\\nhuman binary feedback (\\n or\\n ), or predicted', 'replies with the satisfaction classiﬁer trained in\\nStep 1a. After this step, every bot reply in the\\ndataset has a binary label either from the original\\nhuman binary feedback (\\n or\\n ), or predicted\\nby the satisfaction classiﬁer (\\n or\\n ).\\n4.3 Step 3: Convert lemons to cherries\\nWe can now augment the training data. We use\\nthereply corrector trained in step 1b to generate\\nimproved replies for any examples labeled as bad (\\nor\\n ), and then add them to the training set\\nfor the ﬁnal dialogue model.\\nSelecting correctable cases However, we note\\nthat not all bad responses are easily correctable\\ngiven free-form textual feedback. For example,\\nthe human feedback “You are talking nonsense!”\\ncould help indicate this is a\\n using the satis-\\nfaction classiﬁer , but is less helpful for knowing\\nwhat the right response is, compared to more con-\\nstructive feedback such as “That’s a quick topic\\nchange! Let’s continue to talk about fruit, perhaps\\nfruit drinks?” We thus experiment with detecting', 'what the right response is, compared to more con-\\nstructive feedback such as “That’s a quick topic\\nchange! Let’s continue to talk about fruit, perhaps\\nfruit drinks?” We thus experiment with detecting\\ncases that are “correctable”, and only use these\\nto augment our training data. We ﬁrst embed the\\nfree-form textual feedback and the immediate next\\nbot reply in recorded conversations with Sentence-\\nBERT (Reimers and Gurevych, 2019), and then\\ncalculate their cosine similarity score. If the score\\nis high, it means that the human free-form textual\\nfeedback is easier for a model to comprehend and\\nthus revise its own response accordingly. We deﬁne\\nsuch examples as correctable and then threshold\\nthe similarity score to pick out correctable cases.\\nPredicting reply corrections To obtain the cor-\\nrections, we adopt a reranking-based learning\\nmethod widely used in many previous studies (Nie\\net al., 2020; Nakano et al., 2021; Askell et al., 2021)\\nto score and rank the generations. We ﬁrst use the', 'method widely used in many previous studies (Nie\\net al., 2020; Nakano et al., 2021; Askell et al., 2021)\\nto score and rank the generations. We ﬁrst use the\\nreply corrector to generate many correction candi-\\ndates (60 in our experiments). Then we concatenate\\nthe original context with the correction candidates\\nand feed them into the satisfaction classiﬁer from\\nStep 1a. Finally, we select the top one with the\\nhighest probability output by the classiﬁer as the\\nﬁnal correction. If all generated corrections are\\npredicted as bad, we will skip this example.\\n4.4 Step 4: Collect fruits and re-train\\nAfter the previous steps, each bot response is anno-\\ntated with either a gold or predicted binary label,\\nand those labeled as bad are converted from bad\\nresponses to good ones using human feedback. The\\nﬁnal step is to augment the training set of the ﬁnal\\ndialogue model with the new data.\\nOne straightforward method to improve the\\nmodel is to augment the training data with all the', 'ﬁnal step is to augment the training set of the ﬁnal\\ndialogue model with the new data.\\nOne straightforward method to improve the\\nmodel is to augment the training data with all the\\npositive replies including the corrections (\\n +\\n) and use the standard language modeling objective.\\nHowever, this standard training does not utilize\\nnegative/bad replies (\\n +\\n ) to avoid them. Wehence also apply the recently proposed DIRECTOR\\nmodel (Arora et al., 2022) to both reinforce the\\npositive responses and penalize the negative ones.\\nDIRECTOR is a uniﬁed decoder-classiﬁer model\\njointly trained with a language modeling task and\\na classiﬁcation task. During inference, it uses its\\nlanguage modeling head to predict the next token\\nprobability, and its classiﬁer head to decide if the\\ntokens belong to positive examples to generate the\\nﬁnal output. But it is worth noting that in this step\\ninJUICER , we could use any other approach that\\nutilizes both positive and negative responses to re-', 'ﬁnal output. But it is worth noting that in this step\\ninJUICER , we could use any other approach that\\nutilizes both positive and negative responses to re-\\ntrain and improve the ﬁnal dialogue model.\\n5 Experimental Setup\\nIn our experiments, we used the 3B parameter\\nBlenderBot2 (BB2 3B) (Komeili et al., 2021; Xu\\net al., 2021) as the base dialogue model and try to\\nimprove it with human feedback from deployment.\\n5.1 Datasets: FITS and DEMO\\nWe performed experiments on the FITS (Xu et al.,\\n2022) dataset. We also tested the zero-shot trans-\\nferability of both the satisfaction classiﬁers and\\nthereply correctors on a real deployment dataset\\nDEMO (Ju et al., 2022).\\n5.1.1 FITS\\nFITS contains internet-augmented human-bot dia-\\nlogues with annotated feedback for every turn, in-\\ncluding a binary label, free-form textual feedback\\nand a gold response, with around 39k bot utter-\\nances in total. See Section A.1 for more details. To\\nmimic a deployment setting with limited feedback,', 'and a gold response, with around 39k bot utter-\\nances in total. See Section A.1 for more details. To\\nmimic a deployment setting with limited feedback,\\nwe uniformly sampled 20% of the bot responses\\nfrom the training set of FITS and considered them\\nto have binary feedback and gold labels, while the\\nrest were considered unlabeled. However, we did\\nnot remove free-form textual feedback when it is\\npresent, as it remains part of the conversation, see\\nFigure 1. Table 4 in the Appendix shows the data\\nstatistics after sampling.\\nWe used the original FITS validation, test set\\nand unseen test set (of new conversational topics)\\nfor evaluation, and employed the same metrics as\\nXu et al. (2022) for the ﬁnal dialogue models: per-\\nplexity, F1 overlap with the gold annotation, and\\nhuman evaluation via conversations with the bot.\\nDuring conversations, crowdworkers click\\n or\\nper turn and give a ﬁnal rating (a score out of\\n5) in the end. We report the average good response\\nrate in percentage.5.1.2 DEMO', 'During conversations, crowdworkers click\\n or\\nper turn and give a ﬁnal rating (a score out of\\n5) in the end. We report the average good response\\nrate in percentage.5.1.2 DEMO\\nThe dataset DEMO is from the deployment of\\nBlenderBot 3 (Shuster et al., 2022b) with responses\\nveriﬁed by crowdworkers (Ju et al., 2022). In total\\n923 bot responses across 81 conversations are used\\nas an evaluation set.\\n5.2 Baselines\\nWe have two categories of baselines: (1) with-\\nout model-augmented data, and (2) with a prompt-\\nbased reply corrector . In addition, we also compare\\nwith oracle methods using 100% labeled feedback\\ndata without sampling.\\nBaselines without augmentation. The most\\nstraightforward baselines are to ﬁne-tune with the\\nlimited human-labeled feedback only.\\n•Gold corrections from 20% Gold correc-\\ntions provide a strong learning signal. Here,\\nwe simply ﬁne-tune BB2 3B on the 20% gold\\ncorrections.\\n•Free-form textual feedback from 20% Fol-\\nlowing Hancock et al. (2019), we ﬁne-tune', 'tions provide a strong learning signal. Here,\\nwe simply ﬁne-tune BB2 3B on the 20% gold\\ncorrections.\\n•Free-form textual feedback from 20% Fol-\\nlowing Hancock et al. (2019), we ﬁne-tune\\nBB2 3B with the context as the input and the\\nfree-form textual feedback (identiﬁed as the\\nresponse following the bad\\n responses) as\\nthe target.\\nBaseline with a prompt-based reply corrector .\\nInstead of training a supervised reply corrector\\nwith gold corrections, this baseline prompts an off-\\nthe-shelf model with free-form textual feedback\\nand instructions like “given the feedback, correct\\nthe original response” as a reply corrector to gen-\\nerate corrections, and then ﬁne-tunes the ﬁnal dia-\\nlogue model on these corrections.\\n•3B-all-corrections : Scheurer et al. (2022)\\nproposed an approach to improve language\\nmodels with language feedback, originally ap-\\nplied to summarization tasks, which we adapt\\nhere for dialogue. Given a small number (100)\\nhuman feedback samples, they prompted a', 'models with language feedback, originally ap-\\nplied to summarization tasks, which we adapt\\nhere for dialogue. Given a small number (100)\\nhuman feedback samples, they prompted a\\nlanguage model to condition on the context\\n(input+feedback) to re-generate multiple sum-\\nmarization corrections, picked the correc-\\ntion with the highest similarity score with\\nthe feedback, and ﬁnally ﬁne-tuned the lan-\\nguage model on the corrections to improve\\nit. In our implementation, we use the base-\\nline BlenderBot 2 model (3B) as the replycorrector . While Scheurer et al. (2022) used\\nlarger language models (175B), our implemen-\\ntation of the baseline is more comparable to\\nourJUICER models since our reply corrector\\nalso has 3B parameters. In our experiments,\\ninstead of using only 100 examples, we make\\nthis a stronger baseline by generating correc-\\ntions for allthe bad replies.\\n5.3 J UICER models\\nWe also compare several variants of J UICER .\\n•JUICER . We ﬁne-tune BB2 3B by aug-', 'this a stronger baseline by generating correc-\\ntions for allthe bad replies.\\n5.3 J UICER models\\nWe also compare several variants of J UICER .\\n•JUICER . We ﬁne-tune BB2 3B by aug-\\nmenting the 20% human-annotated data with\\n(1) predicted good responses by the satisfac-\\ntion classiﬁer from the remaining 80% un-\\nannotated turns, and (2) predicted corrections\\ngenerated by the reply corrector , ﬁltered to\\nonly include the correctable cases rather than\\nusing all the predicted corrections.\\n•JUICER +DIRECTOR . We ﬁne-tune using\\nDIRECTOR which uses both the positive and\\nnegative replies. Both gold annotations and\\nthe ﬁltered corrections generated by the reply\\ncorrector are used as positive classiﬁcation\\ndata. Bad responses labeled by humans or\\nthesatisfaction classiﬁer are used as negative\\ndata for ﬁne-tuning the classiﬁer head.\\n•w/o predicted corrections (from Step 3) . In\\nthis ablation, we ﬁne-tune the ﬁnal dialogue\\nmodel with only predicted good responses by', 'data for ﬁne-tuning the classiﬁer head.\\n•w/o predicted corrections (from Step 3) . In\\nthis ablation, we ﬁne-tune the ﬁnal dialogue\\nmodel with only predicted good responses by\\nthesatisfaction classiﬁer , without the correc-\\ntions generated by the reply corrector .\\n•w/o selecting correctable cases . In this abla-\\ntion, we only augment with (1) predicted good\\nresponses by the satisfaction classiﬁer , and (2)\\nall the predicted corrections without selecting\\nthe more correctable cases. This tests if select-\\ning correctable cases brings improvements.\\n6 Results\\nWe ﬁrst evaluate the satisfaction classiﬁer (Ta-\\nble 1a), and the reply corrector (Table 1b). We\\nthen perform both automatic and human evalua-\\ntions on the ﬁnal dialogue models (Table 2 and\\nTable 3).\\n6.1 Satisfaction classiﬁer\\nTable 1a shows the classiﬁers’ performance on the\\nFITS data and also their zero-shot performance on\\nDEMO.Adding the next human response helps. We\\nﬁnd the balanced accuracy of detecting satisfac-', 'Table 1a shows the classiﬁers’ performance on the\\nFITS data and also their zero-shot performance on\\nDEMO.Adding the next human response helps. We\\nﬁnd the balanced accuracy of detecting satisfac-\\ntion using only the dialogue context and the bot\\nresponse itself is\\x1875% on FITS. It is signiﬁcantly\\nimproved to\\x1895% by including the next human\\nmessage in the input. A similar improvement is\\nfound when measuring balanced F1 as well. On the\\ndeployment dataset DEMO where organic users are\\nnot required to always write free-form textual feed-\\nback when seeing a bad reply, adding the human\\nresponse still improves the balanced F1 from 64.77\\nto 71.24, despite this being zero-shot performance\\n(without training on this dataset). These results\\nindicate the importance of using the next human\\nmessage to make satisfaction classiﬁcation deci-\\nsions. As using the next human response helps, we\\ndefault to using this satisfaction classiﬁer variant\\nin our standard J UICER setup.\\n6.2 Reply corrector', 'sions. As using the next human response helps, we\\ndefault to using this satisfaction classiﬁer variant\\nin our standard J UICER setup.\\n6.2 Reply corrector\\nTable 1b shows the results of training the reply cor-\\nrector , comparing different input feature choices.\\nFree-form textual feedback improves the cor-\\nrection. We performed an ablation study where\\nthereply corrector trains on (context + bad reply !\\ngood reply ) without the free-form textual feedback,\\nshown in “w/o free-form textual feedback”. As ex-\\npected, adding free-form textual feedback on what\\nwent wrong improves the reply corrector ’s perfor-\\nmance. The best results are relatively close to the\\noracle performance which uses 100% (rather than\\n20%) gold data for training (23.39 F1 vs. 21.41\\nand 2.93 PPL vs. 3.07).\\nAugmenting with self-correction pairs helps.\\nThe standard reply corrector trains on “gold-\\ncorrection” pairs (context + bad reply + free-form\\ntextual feedback!gold correction). Besides these', 'Augmenting with self-correction pairs helps.\\nThe standard reply corrector trains on “gold-\\ncorrection” pairs (context + bad reply + free-form\\ntextual feedback!gold correction). Besides these\\nhuman-written gold corrections, we can also train\\nthereply corrector on “self-correction” pairs (con-\\ntext + bad reply + free-form textual feedback !\\ngood bot reply ), where a bad reply is followed by\\na good bot reply either liked by humans or pre-\\ndicted as good by the satisfaction classiﬁer , sug-\\ngesting that the bot “corrects” itself in the follow-\\ning turn. We found that augmenting with these\\n“self-corrections” improves the F1 from 17.10 to\\n21.41. We can also multitask with various dialogue\\ntasks to further improve the reply corrector ’s per-\\nformance. See Section A.4.1 for more details.(1a) Satisfaction Classiﬁer Valid Test Test Unseen DEMO (zero-shot)\\nInput Acc\" F1\" Acc\" F1\" Acc\" F1\" Acc\" F1\"\\ncontext+bot+human 94.66 97.25 95.76 97.83 96.74 98.34 59.73 71.24', 'Input Acc\" F1\" Acc\" F1\" Acc\" F1\" Acc\" F1\"\\ncontext+bot+human 94.66 97.25 95.76 97.83 96.74 98.34 59.73 71.24\\ncontext+bot 75.58 86.07 74.53 85.38 71.46 83.25 56.60 64.77\\n(a)Satisfaction classiﬁer results (classiﬁcation balanced accuracy and balanced f1) on both FITS and DEMO (zero-shot).\\nAdding the next human message helps the satisfaction prediction, even in the zero-shot case.\\n(1b) Reply Corrector Valid Test Test Unseen\\nInput F1\" PPL# F1\" PPL# F1\" PPL#\\nOracle with 100% annotations\\ngold corrections from 100% 23.39 2.93 21.83 2.63 22.27 4.56\\nw/ free-form textual feedback\\ngold corrections from 20% + self-corrections 21.41 3.07 20.20 2.75 21.77 4.66\\ngold corrections from 20% 17.10 3.37 16.21 2.98 17.91 4.97\\nw/o free-form textual feedback\\ngold corrections from 20% + self-corrections 18.80 3.13 18.36 2.82 18.97 4.84\\ngold corrections from 20% 16.41 3.40 15.08 3.04 16.46 5.06', 'w/o free-form textual feedback\\ngold corrections from 20% + self-corrections 18.80 3.13 18.36 2.82 18.97 4.84\\ngold corrections from 20% 16.41 3.40 15.08 3.04 16.46 5.06\\n(b)Reply corrector perplexity and F1 on valid/test/test unseen sets. Augmenting with self-corrections improves the result,\\ncomparable to the oracle model using 100% gold corrections. Using free-form textual feedback is helpful.\\nTable 1: Performance of the modules in Step 1: (a) satisfaction classiﬁer , and (b) reply corrector .\\nQualitative results show the corrections make\\nsense. We also include generated correction ex-\\namples on the FITS data in Appendix Table 8 and\\non the deployment data in a zero-shot fashion in\\nAppendix Table 9. These examples show that the\\nreply corrector can integrate free-form textual feed-\\nback to correct the bad replies, even for zero-shot\\ndeployment data.\\nSee section A.4.2 for further details and results\\non the reply corrector evaluation.\\n6.3 Final dialogue model evaluations', 'back to correct the bad replies, even for zero-shot\\ndeployment data.\\nSee section A.4.2 for further details and results\\non the reply corrector evaluation.\\n6.3 Final dialogue model evaluations\\nThe ﬁnal dialogue model results are given in Ta-\\nble 2 (automatic evaluations) and Table 3 (human\\nevaluations). All methods are ﬁne-tuned from the\\n3B parameter BlenderBot 2 (BB2), making the\\nmodels comparable.\\nUsing J UICER to augment data improves re-\\nsults. JUICER yields signiﬁcant gains over the\\nbaseline transformer BB2 3B in both automatic\\nevaluations and human evaluations. For example,\\nwe see an F1 increase from 15.3 to 18.5 on the\\nunseen test set, and an improvement of good re-\\nsponses from 33.2% to 41.9% in human evalua-\\ntions. JUICER also performs better than baselines\\nwithout augmentation (e.g., gold corrections from\\n20%).Our supervised reply corrector outperforms a\\nprompt-based one. Compared to the prompt-\\nbased reply corrector baseline (Scheurer et al.,', 'without augmentation (e.g., gold corrections from\\n20%).Our supervised reply corrector outperforms a\\nprompt-based one. Compared to the prompt-\\nbased reply corrector baseline (Scheurer et al.,\\n2022), all the JUICER models perform better in\\nautomatic evaluations. When the prompt-based\\nmodel is used as a reply corrector to produce cor-\\nrections to augment the ﬁnal dialogue model train-\\ning, the ﬁnal model evaluation (F1=14.2, ppl=8.9)\\nis worse than augmenting with the corrections in\\nJUICER (F1=16.7, ppl=8.5).\\nAugmenting training with predicted correc-\\ntions in J UICER helps. JUICER augments train-\\ning with predicted corrections, which improves\\nboth the F1 and perplexity across the board com-\\npared to JUICER without predicted corrections, e.g.\\n18.5 vs. 17.9 on the test unseen F1. This makes\\nsense because the predicted corrections are gener-\\nated by the reply corrector given human free-form\\ntextual feedback which contains valuable informa-\\ntion, and ﬁne-tuning the ﬁnal dialogue model on', 'sense because the predicted corrections are gener-\\nated by the reply corrector given human free-form\\ntextual feedback which contains valuable informa-\\ntion, and ﬁne-tuning the ﬁnal dialogue model on\\nthese corrections can steer it toward better replies.\\nSelecting correctable cases can help. JUICER\\npicks only correctable cases to augment the training\\ndata, with around 62% of cases selected (threshold\\nchosen based on the validation set). Compared to\\nnaively augmenting with all predicted corrections,\\nwe see gains on valid and unseen test F1 (18.5 vs.Final dialogue model Automatic evaluation\\nValid Test Test Unseen\\nF1\"PPL#F1\"PPL#F1\"PPL#\\nBB2 3B 14.4 10.6 14.7 10.3 15.3 9.3\\n+gold corrections from 20% 16.2 9.1 15.6 8.9 17.9 8.4\\n+free-form textual feedback from 20% 13.1 10.4 12.6 10.3 13.7 9.6\\n3B-all-corrections (prompt-based) 14.2 8.9 14.5 8.7 15.2 8.2\\nJUICER models\\n+JUICER 16.7 8.5 16.2 8.4 18.5 8.0\\n+JUICER +DIRECTOR 17.2 n/a 16.7 n/a 17.7 n/a\\nJUICER ablations', '3B-all-corrections (prompt-based) 14.2 8.9 14.5 8.7 15.2 8.2\\nJUICER models\\n+JUICER 16.7 8.5 16.2 8.4 18.5 8.0\\n+JUICER +DIRECTOR 17.2 n/a 16.7 n/a 17.7 n/a\\nJUICER ablations\\nw/o predicted corrections 15.7 9.0 15.8 8.8 17.9 8.2\\nw/o selecting correctable cases 16.4 8.5 16.4 8.4 18.0 8.1\\nTable 2: Final dialogue model automatic evaluation results. All the dialogue models are ﬁne-tuned from BB2 3B.\\nJUICER models with augmentations are better than the baselines without augmentations. J UICER with a supervised\\nreply corrector also performs better than the baseline with a prompted-based reply corrector . DIRECTOR utilizing\\nnegative examples is effective. Using predicted corrections and selecting correctable cases are useful.\\nFinal dialogue model Human evaluation\\nGood%\"Rating\"\\nBB2 3B 33.2% 3.09\\n+gold corrections from 20% 39.4% 2.89\\nJUICER models\\n+JUICER 41.9% 3.06\\n+JUICER +DIRECTOR 45.5% 3.34\\nTable 3: Final dialogue model human evaluation results.', 'Good%\"Rating\"\\nBB2 3B 33.2% 3.09\\n+gold corrections from 20% 39.4% 2.89\\nJUICER models\\n+JUICER 41.9% 3.06\\n+JUICER +DIRECTOR 45.5% 3.34\\nTable 3: Final dialogue model human evaluation results.\\nWe report the % of good responses and the overall rat-\\ning, as judged by crowdworkers during conversations.\\nWe bold statistically signiﬁcant improvements (inde-\\npendent two-sample t-test,p < 0:05) of methods over\\nthe BB2 3B baseline. J UICER outperforms the base-\\nlines. J UICER +DIRECTOR works the best.\\n18.0), although there is no gain on the seen test set.\\nDIRECTOR provides further gains. DIREC -\\nTOR utilizes both the (predicted) binary feedback\\nsignal and textual feedback signal to avoid negative\\nresponses. Applying it improves the results fur-\\nther over standard JUICER (45.5% good responses\\nvs. 41.9% for JUICER without DIRECTOR , as mea-\\nsured by human evaluations). Because DIRECTOR\\nuses a classiﬁer head to decide if a token should\\nbe included in the ﬁnal generation, the distribution', 'vs. 41.9% for JUICER without DIRECTOR , as mea-\\nsured by human evaluations). Because DIRECTOR\\nuses a classiﬁer head to decide if a token should\\nbe included in the ﬁnal generation, the distribution\\nis altered and perplexity measures are not applica-\\nble. However, it gives gains in F1 on valid and test\\nsets, although not on the unseen test set. JUICER\\nandDIRECTOR together also outperforms DIREC -\\nTOR alone, even when DIRECTOR uses 100% gold\\nbinary labels, see Appendix Table 11. Furthervariants and experiments with DIRECTOR are also\\ngiven in Section A.5.2.\\nJUICER achieves comparable results to meth-\\nods with oracle access to gold labels. Com-\\npared to methods using 100% gold data which\\nwas not given to J UICER , our best J UICER models\\nachieve comparable performance, especially on F1\\nand human evaluation. For example, test unseen\\nF1=17.6 for the best “oracle” method vs. 17.7 for\\nthe best JUICER model, 47.0% vs. 45.5% good\\nresponses, and 3.38 vs. 3.34 in human ratings. See', 'and human evaluation. For example, test unseen\\nF1=17.6 for the best “oracle” method vs. 17.7 for\\nthe best JUICER model, 47.0% vs. 45.5% good\\nresponses, and 3.38 vs. 3.34 in human ratings. See\\nAppendix Table 11 for further details. These “100%\\ndata” methods can be seen as upper bound results,\\nshowing that JUICER does extract most of the sig-\\nnal from the portion of the dialogue data without\\nbinary or gold feedback.\\nSee Section A.5 for further experiments and de-\\ntails on ﬁnal model evaluations.\\n7 Conclusion\\nDeployed dialogue agents should continuously im-\\nprove by using human feedback gathered during\\ninteractions. Unfortunately, feedback collected in\\nthe wild can be limited. In this paper, we proposed\\nJUICER , a framework to efﬁciently use limited or-\\nganic feedback signals (binary labels and gold cor-\\nrections) if free-form textual feedback is provided.\\nJUICER works by correcting bad responses into\\ngood ones to augment the training data for the ﬁ-', 'rections) if free-form textual feedback is provided.\\nJUICER works by correcting bad responses into\\ngood ones to augment the training data for the ﬁ-\\nnal dialogue model. Experiments show that aug-\\nmenting with such predictions can integrate human\\nfeedback and improve overall performance.8 Limitations and Discussions\\nIn our experimental setting, we assume dense free-\\nform textual feedback, i.e., a bad reply is always\\nfollowed by a free-form message explaining what\\nwas wrong. In real deployments, this free-form\\ntextual feedback signal may not always be given\\nand without it, the binary satisfaction classiﬁer\\nmay not necessarily achieve a high accuracy or\\nF1 (e.g., 90+), which could also impact the later\\nsteps. It remains to be seen in real deployments\\nhow dense this signal is, and what methods can be\\nused to encourage users to make these signals as\\ndense as possible, so that strong feedback signals\\nare available to train on.\\nWe have also assumed good intent from human', 'used to encourage users to make these signals as\\ndense as possible, so that strong feedback signals\\nare available to train on.\\nWe have also assumed good intent from human\\nconversationalists, but it is possible to have adver-\\nsarial and bad actors interacting with the bot. In\\nparticular, incorrect feedback or opposite feedback\\n(e.g., thumbs up instead of down) could be supplied\\nby the human for incorrect bot behavior. We see\\nthis as an important research direction that should\\nbe pursued in parallel to work on algorithms like\\nthe ones we study here. See e.g. Ju et al. (2022) for\\nrecent work addressing bad actors and adversarial\\nfeedback.\\nThe training/evaluation loop of JUICER can be\\nlong due to its iterative nature. The advantage of\\nusing a reply corrector is that we can qualitatively\\nevaluate the quality of the generated corrections.\\nBut the drawback is that we need to ﬁrst train a\\nreply corrector , use it to generate corrections, and\\nﬁnally improve the dialogue. We assume that the', 'But the drawback is that we need to ﬁrst train a\\nreply corrector , use it to generate corrections, and\\nﬁnally improve the dialogue. We assume that the\\nbest reply corrector will lead to the best ﬁnal dia-\\nlogue model, but this remains to be studied. An-\\nother possible direction is to use a latent reply cor-\\nrector to integrate the feedback in a more end-to-\\nend fashion instead of a supervised reply corrector\\nthat will generate explicit corrections separate from\\nthe dialogue model.\\nAdditionally, the proposed JUICER framework\\nimproves the dialogue model ofﬂine rather than cor-\\nrecting the response on the ﬂy. With the necessary\\ninfrastructure support, there is potential for improv-\\ning the models online. This could be a natural\\nsetting for reinforcement learning to get interactive\\nfeedback and iteratively update the model policy\\nas the conversation continues. Such a direction\\ndoes not come without dangers, however, such as\\nthe model degrading if it receives poor inputs, e.g.', 'feedback and iteratively update the model policy\\nas the conversation continues. Such a direction\\ndoes not come without dangers, however, such as\\nthe model degrading if it receives poor inputs, e.g.\\nfrom bad actors as mentioned before.References\\nKushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and\\nJason Weston. 2022. Director: Generator-classiﬁers\\nfor supervise language modeling. arXiv preprint\\narXiv: .\\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn\\nDrain, Deep Ganguli, Tom Henighan, Andy Jones,\\nNicholas Joseph, Ben Mann, Nova DasSarma, et al.\\n2021. A general language assistant as a laboratory\\nfor alignment. arXiv preprint arXiv:2112.00861 .\\nJason Baumgartner, Savvas Zannettou, Brian Kee-\\ngan, Megan Squire, and Jeremy Blackburn. 2020.\\nThe pushshift reddit dataset. arXiv preprint\\narXiv:2001.08435 .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda', 'arXiv:2001.08435 .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems , 33:1877–1901.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\\ncross-lingual representation learning at scale. arXiv\\npreprint arXiv:1911.02116 .\\nAhmed Elgohary, Christopher Meek, Matthew\\nRichardson, Adam Fourney, Gonzalo Ramos, and\\nAhmed Hassan Awadallah. 2021. Nl-edit: Correct-\\ning semantic parse errors through natural language\\ninteraction. ACL.\\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\\nMazare, and Jason Weston. 2019. Learning from\\ndialogue after deployment: Feed yourself, chatbot!\\narXiv preprint arXiv:1901.05415 .\\nDa Ju, Jing Xu, Y-Lan Boureau, and Jason Weston.', 'Mazare, and Jason Weston. 2019. Learning from\\ndialogue after deployment: Feed yourself, chatbot!\\narXiv preprint arXiv:1901.05415 .\\nDa Ju, Jing Xu, Y-Lan Boureau, and Jason Weston.\\n2022. Learning from data in the mixed adversarial\\nnon-adversarial case: Finding the helpers and ignor-\\ning the trolls. arXiv preprint arXiv .\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\n2021. Internet-augmented dialogue generation.\\narXiv preprint arXiv:2107.07566 .\\nJiwei Li, Alexander H Miller, Sumit Chopra,\\nMarc’Aurelio Ranzato, and Jason Weston. 2016a.\\nDialogue learning with human-in-the-loop. arXiv\\npreprint arXiv:1611.09823 .\\nJiwei Li, Alexander H Miller, Sumit Chopra,\\nMarc’Aurelio Ranzato, and Jason Weston. 2016b.\\nLearning through dialogue interactions by asking\\nquestions. arXiv preprint arXiv:1612.04936 .\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,', 'Learning through dialogue interactions by asking\\nquestions. arXiv preprint arXiv:1612.04936 .\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,et al. 2021. Webgpt: Browser-assisted question-\\nanswering with human feedback. arXiv preprint\\narXiv:2112.09332 .\\nYixin Nie, Mary Williamson, Mohit Bansal, Douwe\\nKiela, and Jason Weston. 2020. I like ﬁsh, espe-\\ncially dolphins: Addressing contradictions in dia-\\nlogue modeling. arXiv preprint arXiv:2012.13391 .\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow in-\\nstructions with human feedback. arXiv preprint\\narXiv:2203.02155 .\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nbert: Sentence embeddings using siamese bert-\\nnetworks. arXiv preprint arXiv:1908.10084 .', 'arXiv:2203.02155 .\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nbert: Sentence embeddings using siamese bert-\\nnetworks. arXiv preprint arXiv:1908.10084 .\\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven\\nBills, Long Ouyang, Jonathan Ward, and Jan Leike.\\n2022. Self-critiquing models for assisting human\\nevaluators. arXiv preprint arXiv:2206.05802 .\\nJérémy Scheurer, Jon Ander Campos, Jun Shern Chan,\\nAngelica Chen, Kyunghyun Cho, and Ethan Perez.\\n2022. Training language models with language feed-\\nback. The First Workshop on Learning with Natural\\nLanguage Supervision at ACL 2022 .\\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\\nPetroni, Patrick Lewis, Gautier Izacard, Qingfei You,\\nChristoforos Nalmpantis, Edouard Grave, and Se-\\nbastian Riedel. 2022. Peer: A collaborative lan-\\nguage model. arXiv preprint arXiv:2208.11663 .\\nAbigail See and Christopher D Manning. 2021. Under-\\nstanding and predicting user dissatisfaction in a neu-\\nral generative chatbot. In Proceedings of the 22nd', 'Abigail See and Christopher D Manning. 2021. Under-\\nstanding and predicting user dissatisfaction in a neu-\\nral generative chatbot. In Proceedings of the 22nd\\nAnnual Meeting of the Special Interest Group on Dis-\\ncourse and Dialogue , pages 1–12.\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\\nStephen Roller, Arthur Szlam, and Jason We-\\nston. 2022a. Language models that seek for\\nknowledge: Modular search & generation for di-\\nalogue and prompt completion. arXiv preprint\\narXiv:2203.13224 .\\nKurt Shuster, Jack Urbanek, Emily Dinan, Arthur\\nSzlam, and Jason Weston. 2020. Deploying life-\\nlong open-domain dialogue learning. arXiv preprint\\narXiv:2008.08076 .\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\\nEric Michael Smith, Stephen Roller, Megan Ung,\\nMoya Chen, Kushal Arora, Joshua Lane, et al.\\n2022b. Blenderbot 3: a deployed conversational\\nagent that continually learns to responsibly engage.\\narXiv preprint arXiv:2208.03188 .\\nEric Smith, Mary Williamson, Kurt Shuster, Jason We-', '2022b. Blenderbot 3: a deployed conversational\\nagent that continually learns to responsibly engage.\\narXiv preprint arXiv:2208.03188 .\\nEric Smith, Mary Williamson, Kurt Shuster, Jason We-\\nston, and Y-Lan Boureau. 2020. Can you put it all\\ntogether: Evaluating conversational agents’ ability\\nto blend skills. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-\\nguistics . ACL.\\nNiket Tandon, Aman Madaan, Peter Clark, and Yim-\\ning Yang. 2022. Learning to repair: Repairing\\nmodel output errors after deployment using a dy-\\nnamic memory of feedback. NAACL Findings.(to\\nappear) .\\nJing Xu, Arthur Szlam, and Jason Weston. 2021. Be-\\nyond goldﬁsh memory: Long-term open-domain\\nconversation. arXiv preprint arXiv:2107.07567 .\\nJing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora,\\nY-Lan Boureau, and Jason Weston. 2022. Learn-\\ning new skills after deployment: Improving open-\\ndomain internet-driven dialogue with human feed-\\nback. arXiv preprint arXiv:2208.03270 .A Appendix', 'Y-Lan Boureau, and Jason Weston. 2022. Learn-\\ning new skills after deployment: Improving open-\\ndomain internet-driven dialogue with human feed-\\nback. arXiv preprint arXiv:2208.03270 .A Appendix\\nA.1 Details on FITS\\nIn this section, we describe the existing FITS task\\n(Xu et al., 2022) in more details. In FITS, each\\nbot message is annotated with the following feed-\\nback. The setting also ensures that after the human\\nprovides the feedback, the conversation can be con-\\ntinued with the feedback integrated.\\n• A binary satisfaction label.\\n•If it is a bad reply, the human provides free-\\nform textual feedback on what went wrong in\\nthe next human message.\\n•Multiple-choice selection on what the bot\\ncould do to improve this turn:\\n(a) using a better search query; or\\n(b) attending better to the search results; or\\n(c) other issues with the overall reply; or\\n(d) no issue (a good reply).\\n•If selecting (a), the human provides a better\\nsearch query, which will be used in the next', '(c) other issues with the overall reply; or\\n(d) no issue (a good reply).\\n•If selecting (a), the human provides a better\\nsearch query, which will be used in the next\\nturn to continue the conversation.\\n•If selecting (b), the human is presented with\\nthe search results and selects the relevant sen-\\ntences, which will be added to the model input\\nin the next turn.\\n•If selecting (c), the human provides a bet-\\nter overall reply (a gold correction), which\\nis copied to be the next bot response.\\nA.2 Sampling FITS\\nIn our experiments, we uniformly sample 20% of\\nthe FITS training set to mimic a deployment set-\\nting with sparse binary and gold feedback. Table 4\\nshows the sampled dataset statistics. Out of the\\n20% of the training set with labels, 1376 exam-\\nples are “better overall reply” annotated with gold\\ncorrections, which accounts for 7% of all bad re-\\nsponses to be corrected in FITS. Those 1376 correc-\\ntions will later be used to train the reply corrector', 'corrections, which accounts for 7% of all bad re-\\nsponses to be corrected in FITS. Those 1376 correc-\\ntions will later be used to train the reply corrector\\nfor augmenting limited human feedback. The size\\nof 20% of the FITS training set (7768 examples) is\\nalso similar to that of the validation and test sets.Feedback Breakdown Train (20%) Valid Test Test Unseen\\nTotal 7768 4245 9726 8907\\nBetter Search Query 1056 605 1167 1036\\nBetter Results Usage 1383 756 1527 1310\\nBetter Overall Reply 1376 714 1493 1372\\nGood Response 3953 2170 5539 5189\\nTable 4: Data statistics of the sampled version of FITS\\nused in our experiments. We sampled 20% from FITS.\\nNote that the training set size of labeled binary feed-\\nback is similar to the test sets.\\nA.2.1 Varying the sampling rate\\nAs an ablation study, we varied the sampling rate.\\nTable 5 shows different ﬁnal dialogue models’ re-\\nsults with different sampling rates. The input to the\\ndialogue model is the context, and the output is the', 'Table 5 shows different ﬁnal dialogue models’ re-\\nsults with different sampling rates. The input to the\\ndialogue model is the context, and the output is the\\nhuman-written gold correction.\\nAs we increase the sampling rate, the ﬁnal dia-\\nlogue model’s perplexity improves in general, but\\nthe gain becomes smaller. For instance, when we\\nonly sample 5% from FITS, the validation perplex-\\nity is 9.52; if we increase the sampling rate to 20%,\\nthe perplexity is 9.09; but when we further increase\\nthe sampling rate to 30% and 50%, the perplexity\\nbecomes 9.12 and 8.80 respectively.\\nWe ﬁnd the 20% sampling rate is a good balanc-\\ning point with both reasonable F1 and perplexity.\\nA.3 Different modules in J UICER\\nThere are different modules involved in JUICER\\nand we summarize them in Table 6. To sum up,\\nJUICER has two helper modules, a satisfaction clas-\\nsiﬁer and a reply corrector to help improve the ﬁnal\\ndialogue model.\\nThesatisfaction classiﬁer identiﬁes if the bot re-', 'JUICER has two helper modules, a satisfaction clas-\\nsiﬁer and a reply corrector to help improve the ﬁnal\\ndialogue model.\\nThesatisfaction classiﬁer identiﬁes if the bot re-\\nsponse is satisfactory or not. It is evaluated on both\\nFITS and zero-shot DEMO deployment datasets.\\nBoth the reply corrector and the ﬁnal dialogue\\nmodels are generative models, and are automati-\\ncally evaluated on the human-written gold correc-\\ntions in the FITS validation and test sets, as gold\\ncorrections can reﬂect the model’s ability to gener-\\nate good responses.\\nThereply corrector converts bad responses into\\ngood ones using free-form textual feedback. We\\nevaluate it on gold search results instead of live\\nsearch results, in order to generate better reply\\ncorrections. We describe it in more detail in Sec-\\ntion A.4.\\nThe ﬁnal dialogue model is evaluated on live\\nsearch results from Bing, ﬁltered by Common-Final dialogue model Valid Test Test Unseen\\nVarying the sampling rate F1\"PPL#F1\"PPL#F1\"PPL#', 'tion A.4.\\nThe ﬁnal dialogue model is evaluated on live\\nsearch results from Bing, ﬁltered by Common-Final dialogue model Valid Test Test Unseen\\nVarying the sampling rate F1\"PPL#F1\"PPL#F1\"PPL#\\nBaseline performance varying the sampling rate\\n+gold correction from 20% 16.2 9.1 15.6 8.9 17.9 8.4\\n+gold correction from 5% 16.8 9.5 16.6 9.3 18.9 8.5\\n+gold correction from 10% 16.6 9.3 16.5 9.1 18.9 8.4\\n+gold correction from 30% 15.7 9.1 16.3 8.6 18.1 8.3\\n+gold correction from 50% 16.5 8.8 16.0 9.0 17.9 8.3\\nTable 5: Final dialogue model results varying the sampling rate. Perplexities get better as we increase the sampling\\nrate, but the gain becomes smaller. F1 ﬁrst gets worse and then goes up. These suggest that a sampling rate of 20%\\nis a good balancing point with both a good F1 and a good perplexity.\\nModel Inputs!Outputs Fine-tuned from Evaluated on Description\\n(1a)Satisfaction classiﬁerContext + bot reply (+ the next human\\nresponse)!binary {good, bad} on the', 'Model Inputs!Outputs Fine-tuned from Evaluated on Description\\n(1a)Satisfaction classiﬁerContext + bot reply (+ the next human\\nresponse)!binary {good, bad} on the\\nbot reply311M TransformerFITS valid&test\\nand DEMOGiven the context, a bot reply and poten-\\ntially the next human message, detect if\\nthe bot reply is satisfactory\\n(1b) Reply correctorContext + bad bot reply + free-form tex-\\ntual feedback!improved reply (“a cor-\\nrection”)3B R2C2 (Shuster\\net al., 2022a)Gold corrections in\\nFITS valid&test (on\\ngold search results)Given the context, the bad reply, and\\nfree-form textual feedback, generate an\\nimproved reply (“correct the bad reply”)\\n(2) Final dialogue model Context!reply 3B BlenderBot 2Gold corrections in\\nFITS valid&test (on\\nlive search results)Given the context, generate a reply. We\\nﬁne-tune our models using BlenderBot\\n2 as the base model (Komeili et al.,\\n2021; Xu et al., 2021).\\nTable 6: The input, output, and description of the three models used in J UICER .', 'ﬁne-tune our models using BlenderBot\\n2 as the base model (Komeili et al.,\\n2021; Xu et al., 2021).\\nTable 6: The input, output, and description of the three models used in J UICER .\\nCrawl, following Xu et al. (2022) instead of gold\\nsearch results to better reﬂect performance with live\\nusers. We describe it in more detail in Section A.5.\\nA.4 Reply corrector\\nThereply corrector trains on data where for a given\\nexample the input consists of the dialogue context\\n+ bad reply to correct + the following human mes-\\nsage, and the output consists of the correction for\\nthe bad reply. The models used in the main paper\\nwere ﬁne-tuned from the R2C2 transformer (Shus-\\nter et al., 2022a).\\nA.4.1 Training the reply corrector with\\nmultiple tasks\\nIn our experiments, we multi-tasked with various\\ndialogue tasks to train the reply corrector , which\\nimproves the result. These tasks include the orig-\\ninal reply correction task, the task with context\\nas the input and free-form textual feedback as the', 'improves the result. These tasks include the orig-\\ninal reply correction task, the task with context\\nas the input and free-form textual feedback as the\\ntarget, and the dialogue task of Wizard of Internet\\n(Komeili et al., 2021). We tuned the weights for dif-\\nferent tasks, and other hyper-parameters (learning\\nrate, batch size, etc) according to the performance\\non the validation set.\\nA.4.2 Evaluating the reply corrector\\nSince the reply correctors are used to generate cor-\\nrections rather than interacting with live users, weevaluated them with gold search results (which\\nleads to better corrections) instead of live search\\nresults from Bing (which better reﬂects the live\\ninteraction performance).\\nAlthough the reply corrector (Table 1b) and the\\nﬁnal dialogue models (Table 2) are evaluated on\\nthe same validation and test subsets that have gold\\ncorrections, their results are not comparable be-\\ncause of the following two reasons. First, as men-\\ntioned earlier, the reply correctors condition on', 'corrections, their results are not comparable be-\\ncause of the following two reasons. First, as men-\\ntioned earlier, the reply correctors condition on\\nthe gold search results instead of the live search\\nresults, while the ﬁnal dialogue models use the live\\nsearch results. Second, the reply correctors rely on\\nthe free-form textual feedback to convert lemons\\nto cherries, so we also append the free-form tex-\\ntual feedback into the input to the reply correctors ,\\nbut for the ﬁnal dialogue model, we do not have\\nthe additional free-form textual feedback informa-\\ntion. These are the main reasons why the results in\\nTable 1b are better than those in Table 2.\\nA.4.3 Generating reply corrections\\nWe adopt a reranking-based learning method to\\nﬁrst generate multiple reply corrections, and then\\nuse the satisfaction classiﬁer to score and rerank\\nthe generated corrections. Because the reply cor-\\nrector ’s performance is good (comparable to the', 'use the satisfaction classiﬁer to score and rerank\\nthe generated corrections. Because the reply cor-\\nrector ’s performance is good (comparable to the\\none trained on 100% data in Table 1b) and we gen-(1b) Reply Corrector Valid Test Test Unseen\\nInput F1\" PPL# F1\" PPL# F1\" PPL#\\nﬁne-tuned from R2C2\\ngold corrections from 20% + self-corrections 21.41 3.07 20.20 2.75 21.77 4.66\\n+ D IRECTOR 22.81 - 22.59 - 22.10 -\\n+ D IRECTOR OVERLAP 23.00 - 22.50 - 22.55 -\\nﬁne-tuned from BB2\\ngold corrections from 20% + self-corrections 16.32 7.06 14.53 7.01 15.63 7.16\\nTable 7: Reply corrector results. The top block shows the reply corrector ﬁne-tuned from R2C2 with D IRECTOR\\nand D IRECTOR OVERLAP , and the bottom block shows the reply corrector ﬁne-tuned from BB2. R2C2 is better\\nthan BB2 as a reply corrector . Using D IRECTOR improves the result. Using D IRECTOR OVERLAP further improves\\nover D IRECTOR .\\nerated 60 correction candidates to choose from,\\nthe majority (99.96%, 16893 out of 16989) of bad', 'over D IRECTOR .\\nerated 60 correction candidates to choose from,\\nthe majority (99.96%, 16893 out of 16989) of bad\\nresponses have at least one generated correction\\nthat is predicted as satisfactory by the satisfaction\\nclassiﬁer .\\nGenerated reply correction examples. Table 8\\nand Table 9 show generated reply correction ex-\\namples on the FITS dataset and the deployment\\ndataset (zero-shot) respectively. These qualitative\\nexamples show that the reply corrector can convert\\nbad replies into good ones using free-form textual\\nfeedback, even for unseen deployment data.\\nA.4.4 Using BB2 to train the reply corrector\\nThe models used in the main experiments were ﬁne-\\ntuned from R2C2 (Shuster et al., 2022a). We also\\nreport results ﬁne-tuned with BB2 in Table 7. We\\nﬁnd that BB2 is worse than R2C2 as a reply cor-\\nrector because its generated corrections are more\\nlike conversational replies rather than actual cor-\\nrections.\\nA.4.5 Using D IRECTOR in the reply corrector', 'rector because its generated corrections are more\\nlike conversational replies rather than actual cor-\\nrections.\\nA.4.5 Using D IRECTOR in the reply corrector\\nUsing DIRECTOR to combine multiple feedback\\nsignals is also effective for the reply corrector . We\\ncan use DIRECTOR to further improve the reply cor-\\nrector ’s F1 to 22.81, as shown in Table 7, where the\\npositive examples are the gold corrections and the\\nnegative examples are the bad bot responses. How-\\never, although the F1 of the DIRECTOR -enhanced\\nreply corrector is better, we ﬁnd that if we use it\\nto generate reply corrections to improve the ﬁnal\\ndialogue models, the F1 is slightly better but the\\nperplexity gets worse than using a regular reply cor-\\nrector without DIRECTOR , as shown in Table 10.\\nMore analysis is needed to understand the reasons\\nfor this.A.5 Final dialogue model evaluation\\nWe evaluate the ﬁnal dialogue model on live search\\nresults instead of gold search results to better reﬂect\\nperformance with live users.', 'for this.A.5 Final dialogue model evaluation\\nWe evaluate the ﬁnal dialogue model on live search\\nresults instead of gold search results to better reﬂect\\nperformance with live users.\\nA.5.1 Oracle performance using 100%\\nfeedback data\\nXu et al. (2022) trained various methods on the en-\\ntire FITS dataset. Since our method is trained only\\non 20% of FITS, the 100% models’ performance\\ncould be viewed as an upper bound of our models.\\nThey also used the 3B parameter BlenderBot 2 as\\na base model for the ﬁnal dialogue model, making\\nit comparable to our experiments. Their results are\\nin Table 11 and we detail their models below.\\n•100% gold correction . The input is the con-\\ntext and the target is the gold correction (6,601\\nin the entire FITS dataset). This can be di-\\nrectly compared to “gold correction from 20%”\\nin Table 2.\\n•100% free-form textual feedback . The in-\\nput is the context and the target is the free-\\nform textual feedback. This should be com-\\npared to “free-form textual feedback from', 'in Table 2.\\n•100% free-form textual feedback . The in-\\nput is the context and the target is the free-\\nform textual feedback. This should be com-\\npared to “free-form textual feedback from\\n20%” in Table 2.\\n•100% module supervision . BlenderBot 2\\nis an internet-augmented bot with different\\nmodules such as a search module to gener-\\nate a search query, and a knowledge mod-\\nule to attend to the search results. Using\\nthe human-written gold search query, human-\\nselected search doc and gold correction, they\\nﬁne-tuned each individual module to improve\\nBlenderBot 2.From Utterance\\nUSRHi bot, what supplies do I need for a\\ndiaper change?\\nBOTThat’s a good question. I’m not sure,\\nbut I’m sure you can ﬁnd out online.\\nFeedbackThe results list at least one necessary\\nsupply, with the implication that bot\\nwould ﬁnd others if it read further.\\nCorrectionBring your supplies to the diapering\\narea. Clean cloth diapers are essential.\\nYou’ll also need a clean diaper, clean\\nwipes, diaper cream, gloves, plastic or', 'CorrectionBring your supplies to the diapering\\narea. Clean cloth diapers are essential.\\nYou’ll also need a clean diaper, clean\\nwipes, diaper cream, gloves, plastic or\\nwaterproof bag for soiled clothing, extra\\nclothes.\\nOriginalIt depends on how many diapers you\\nhave. If you have a lot of diapers, you\\nwill need a changing table and a chang-\\ning pad.From Utterance\\nUSR What is the name of hello kitty’s sister?\\nBOT Hello kitty’s twin sister, mimmy.\\nFeedbackThat’s great. I think to make this even\\nbetter it could be phrased a little more\\nconversationally. Something like \"hello\\nkitty has a twin sister, mimmy.\"\\nCorrectionHello kittys sister is called mimmy! I\\nthink it’s funny she looks a lot like her.\\nDo you like hello kitties sister?\\nOriginalHello kitty has a twin sister named\\nmimmy.\\nFrom Utterance\\nUSRCan you tell me what two sides fought\\nduring the war of the roses?\\nBOTThe war of the roses was between the\\nenglish and the french. The english\\nwon.', 'mimmy.\\nFrom Utterance\\nUSRCan you tell me what two sides fought\\nduring the war of the roses?\\nBOTThe war of the roses was between the\\nenglish and the french. The english\\nwon.\\nFeedbackIt looks like that answer is not right. The\\nsearch results say something different.\\nCorrectionThe war was fought between two com-\\npeting royal families: the yorks and the\\nlancasters, both members of the age-old\\nroyal plantagenet family.\\nOriginal The english and the french.From Utterance\\nUSR What are risk factors of asthma?\\nBOTAsthma is a chronic disease of the air\\npassages in the lungs.\\nFeedback Try using a more speciﬁc query\\nCorrectionAsthma can be caused by a combina-\\ntion of genetic and environmental fac-\\ntors. These factors include: early expo-\\nsure to allergens, changing in weather,\\nrespiratory infections and more.\\nOriginalAsthma is a chronic disease of the air\\npassages in the lungs.\\nTable 8: Qualitative correction examples generated by the best reply corrector on FITS.', 'respiratory infections and more.\\nOriginalAsthma is a chronic disease of the air\\npassages in the lungs.\\nTable 8: Qualitative correction examples generated by the best reply corrector on FITS.\\n•100% reward-based learning . They also\\nadopted a reward-based learning approach,\\nand built a satisfaction classiﬁer to identify\\ngood and bad responses. They used the vanilla\\nBlenderBot 2 model to generate multiple re-\\nsponses, then reranked them with the score\\nfrom the classiﬁer as the reward, and chose\\nthe response with the highest reward. Finally,\\nthey ﬁne-tuned BlenderBot 2 on the responses\\nwith high rewards to improve it.\\n•100% D IRECTOR . They also used both the\\nbinary satisfaction labels and the textual feed-\\nback to train DIRECTOR models to further\\nimprove the performance.\\nAs mentioned in the main body of our paper,\\nJUICER achieves comparable performance to the“oracle” (100%) models in F1 and human evalua-\\ntion. For instance, the best oracle method which', 'As mentioned in the main body of our paper,\\nJUICER achieves comparable performance to the“oracle” (100%) models in F1 and human evalua-\\ntion. For instance, the best oracle method which\\nﬁne-tunes individual modules achieves an F1 of\\n17.2 and 47.0% good response rate, and a hu-\\nman rating of 3.38, while the best JUICER model\\nachieves an F1 of 17.2, a good response rate of\\n45.5%, and a similar human rating of 3.34.\\nA.5.2 DIRECTOR OVERLAP\\nWe also develop a new variant of DIRECTOR and\\nuse it to improve the ﬁnal dialogue model. In DI-\\nRECTOR , every token in the positive and negative\\nexamples has a one or zero label respectively. In\\nour setting, we have a pair of a bad response and\\na good response (a gold correction), e.g., “I like\\nwatermelons too! Have you heard of Harry Styles?”\\n(bad) and “I like watermelons too! They tastesFrom Utterance\\nTruncated ...\\nBOTHello! How are you today? I’m doing\\nwell, thank you. What are you up to?\\nUSR I didn’t ask how you are doing?', '(bad) and “I like watermelons too! They tastesFrom Utterance\\nTruncated ...\\nBOTHello! How are you today? I’m doing\\nwell, thank you. What are you up to?\\nUSR I didn’t ask how you are doing?\\nBOTOh, I’m sorry. I meant to say, how are\\nyou and your family doing today?\\nUSR I don’t have a family\\nBOTI’m sorry to hear that. Are you a mem-\\nber of the family motor coach associa-\\ntion (fmca)?\\nFeedbackSomething like: “i’m sorry I assumed\\nyou had a family! I’ll keep this in mind\\ngoing forward”\\nCorrection 1You’re right. I shouldn’t have assumed\\nyou have a family. I will keep this is\\nmind going foward.\\nCorrection 2I apologize. I didn’t mean to assume\\nyou had children! I hope you have a\\ngreat day.From Utterance\\nTruncated ...\\nBOTWhich operators do you like the most?\\nI like the operators from years 1 and 5\\nof siege.\\nUSR I’m not sure, as I’ve never played.\\nBOTHave you played any of the other rain-\\nbow six games? I’ve played rainbow six\\n3 and 4.\\nUSR No.\\nBOTNo, I haven’t. I’ve only played the ﬁrst', 'of siege.\\nUSR I’m not sure, as I’ve never played.\\nBOTHave you played any of the other rain-\\nbow six games? I’ve played rainbow six\\n3 and 4.\\nUSR No.\\nBOTNo, I haven’t. I’ve only played the ﬁrst\\nrainbow six game. Have you played that\\none?\\nFeedbackYou could suggest other games, or ex-\\nplain what rainbow six is about.\\nCorrection 1Rainbow siege is a ﬁrst-person tactical\\nshooter. It is a good game. Do you like\\nit?\\nCorrection 2Do you like any other games? What\\nconsole do you play on? Do you game\\non pc or console?\\nFrom Utterance\\nTruncated ...\\nUSR Try saying good night, or sweet dreams.\\nBOTThanks for your feedback. It is good to\\nknow.\\nUSRAlright, I’m going to bed now. Good\\nnight!\\nBOTThanks! I’ve been trying to get into sci\\nﬁ, but it’s hard to ﬁnd a good book.\\nFeedbackYou could say good night, or something\\nsimilar.\\nCorrection 1Well, you should go to bed so you can\\nsleep well. Good night.\\nCorrection 2 Have a good night!From Utterance\\nBOTDo you know how someone can make\\nthe best pizza?', 'similar.\\nCorrection 1Well, you should go to bed so you can\\nsleep well. Good night.\\nCorrection 2 Have a good night!From Utterance\\nBOTDo you know how someone can make\\nthe best pizza?\\nUSR I don’t, tell me?\\nBOTThere is an international pizza expo in\\nlas vegas that showcases top suppliers\\nand education about pizza.\\nFeedbackThat does not tell me about how to\\nmake the best pizza.\\nCorrection 1The best pizza is made from fresh ingre-\\ndients with high quality toppings and is\\ncooked to perfection. The best sauce is\\nfresh basil.\\nCorrection 2Many pizza shops have training classes\\nto help you learn how to prepare and\\nmake pizza. There are also books to\\nhelp.\\nTable 9: Zero-shot corrections generated by the best reply corrector on unseen deployment data.\\ngreat in drinks.” (good). Since people tend to edit\\nthe original bad response to correct it, they may\\nhave many overlapping tokens (“I like watermel-\\nons too!”), which we do not have to punish. So we\\ndevelop DIRECTOR OVERLAP , where we obtain', 'the original bad response to correct it, they may\\nhave many overlapping tokens (“I like watermel-\\nons too!”), which we do not have to punish. So we\\ndevelop DIRECTOR OVERLAP , where we obtain\\nthe bag of tokens of the pair of the bad response\\nand the gold correction, and assign a positive label\\nfor the overlapping tokens in the negative examples.\\nIn our data, 28.4% of tokens in the bad responses\\noverlap with those in gold corrections (6.5% are\\nstop words and punctuations, and 21.9% are not).Table 7 and Table 12 show the result of DIREC -\\nTOR OVERLAP . For the reply corrector ,DIRECTOR\\nOVERLAP improves the F1 to 23.00 over DIREC -\\nTOR. For the ﬁnal dialogue model, DIRECTOR\\nOVERLAP improves the good response rate and\\nlowers the search result error in human evaluations\\nover D IRECTOR .\\nB Model Training Setting\\nWe use the openly available ParlAI framework for\\nall training runs, as well as for evaluations, whereFinal dialogue model Automatic evaluation\\nValid\\nF1\" PPL#\\nJUICER models', 'B Model Training Setting\\nWe use the openly available ParlAI framework for\\nall training runs, as well as for evaluations, whereFinal dialogue model Automatic evaluation\\nValid\\nF1\" PPL#\\nJUICER models\\n+JUICER 16.7 8.5\\n+JUICER w/ D IRECTOR OVERLAP -based reply corrector 16.8 8.8\\nJUICER ablations\\nw/o selecting correctable cases 16.4 8.5\\nw/o selecting correctable cases w/ D IRECTOR OVERLAP -based reply corrector 16.5 8.7\\nTable 10: Final dialogue model automatic evaluation results. The D IRECTOR OVERLAP -enhanced reply corrector\\nachieves the highest F1 on the reply correction task, better than the regular reply corrector (see Table 7). But when\\nwe use it to generate the reply corrections to further improve the ﬁnal dialogue model, we can improve the F1 of the\\nﬁnal dialogue model slightly, but the perplexity gets a bit worse. Further investigations are needed to understand\\nthe reason for this.\\nOracle model performance Automatic evaluation Human evaluation\\nError Breakdown #', 'the reason for this.\\nOracle model performance Automatic evaluation Human evaluation\\nError Breakdown #\\nValid Test Test Unseen GoodRating\"Search SearchResponseF1\"PPL#F1\"PPL#F1\"PPL#response\" Query Results\\nBB2 14.4 10.6 14.7 10.3 15.3 9.3 33.2% 3.09 12.1% 18.6% 18.1%\\n+100% reward-based learning 15.1 11.0 14.2 10.7 14.3 9.6 36.4% 2.83 11.3% 18.6% 17.0%\\n+100% free-form textual feedback 15.5 9.7 15.6 9.5 16.8 8.7 37.0% 3.22 11.6% 17.6% 17.0%\\n+100% gold correction 14.7 8.2 15.5 8.0 17.0 8.0 40.3% 3.37 11.6% 18.3% 15.0%\\n+100% module supervision 14.9 7.6 15.5 7.5 15.4 8.3 42.0% 3.35 8.4% 20.8% 14.4%\\n+100% reranking binary feedback 15.8 n/a 15.8 n/a 16.3 n/a - - - - -\\n+100% D IRECTOR binary feedback only 16.2 n/a 16.2 n/a 17.6 n/a 37.8% 3.07 11.4% 17.3% 16.9%\\n+100% D IRECTOR module+binary feedback 17.2 n/a 16.6 n/a 16.0 n/a 47.0% 3.38 8.4% 16.1% 14.3%\\nTable 11: Final dialogue model results from 100% oracle methods in Xu et al. (2022). Similarly we bold statisti-', 'Table 11: Final dialogue model results from 100% oracle methods in Xu et al. (2022). Similarly we bold statisti-\\ncally signiﬁcant improvements (independent two-sample t-test,p< 0:05) of methods over their baselines BB2 3B\\nin the human evaluation block.\\nFinal dialogue model Automatic evaluation Human evaluation\\nError Breakdown #\\nValid Test Test Unseen GoodRating\"Search SearchResponseF1\" PPL# F1\" PPL# F1\" PPL#response\" Query Results\\nJUICER\\n+JUICER 16.74 8.50 16.18 8.44 18.50 8.02 41.9% 3.06 13.0% 17.7% 13.8%\\n+JUICER + D IRECTOR 17.25 - 16.70 - 17.70 - 45.5% 3.34 11.3% 17.4% 12.9%\\n+JUICER + D IRECTOR OVERLAP 17.32 - 16.66 - 17.62 - 47.8% 3.25 11.0% 14.8% 13.3%\\nJUICER w/o selecting correctable cases\\n+JUICER 16.44 8.54 16.37 8.41 17.95 8.12 41.4% 3.08 13.4% 16.8% 14.2%\\n+JUICER + D IRECTOR 17.23 - 16.62 - 17.93 - 44.6% 3.40 11.6% 16.7% 13.6%\\n+JUICER + D IRECTOR OVERLAP 16.98 - 16.56 - 17.19 - 45.5% 3.48 10.8% 15.2% 14.3%', '+JUICER + D IRECTOR 17.23 - 16.62 - 17.93 - 44.6% 3.40 11.6% 16.7% 13.6%\\n+JUICER + D IRECTOR OVERLAP 16.98 - 16.56 - 17.19 - 45.5% 3.48 10.8% 15.2% 14.3%\\nTable 12: J UICER with D IRECTOR OVERLAP . DIRECTOR OVERLAP improves the human evaluation results over\\nthe vanilla D IRECTOR . Similarly we bold statistically signiﬁcant improvements (independent two-sample t-test,\\np< 0:05) of methods over their baselines BB2 3B in the human evaluation block.\\nmetrics are measured using default settings. All\\nthe ﬁne-tuned models are trained with a maximum\\nof eight 32GB GPUs (NVIDIA V100), optimized\\nwith Adam using \\x0c1= 0:9,\\x0c2= 0:999,\\x0f=\\n1e\\x0008. Models are trained up to 4000 updates\\nwith batch sizes up to 128. The typical ﬁne-tuning\\ntime for a standard transformer encoder-decoder is8 hrs before it early stops, and the time is 16 hrs\\nfor retrieval-based models.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi5B9cKidg4n",
        "outputId": "bd57c6e4-8b28-44d4-b8de-a021049e5494"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunk[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4hSVyyHeMHs",
        "outputId": "78b74951-82db-4d3a-cdd2-f9d5f9fefc8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When Life Gives You Lemons\n",
            " , Make Cherryade\n",
            " :\n",
            "Converting Feedback from Bad Responses into Good Labels\n",
            "Weiyan Shiy\n",
            "Meta AI & Columbia UniversityEmily Dinan\n",
            "Meta AIKurt Shuster\n",
            "Meta AI\n",
            "Jason Weston\u0003\n",
            "Meta AIJing Xu\u0003\n",
            "Meta AI\n",
            "Abstract\n",
            "Deployed dialogue agents have the potential\n",
            "to integrate human feedback to continuously\n",
            "improve themselves. However, humans may\n",
            "not always provide explicit signals when the\n",
            "chatbot makes mistakes during interactions. In\n",
            "this work, we propose J UICER , a framework to\n",
            "make use of both binary and free-form textual\n",
            "human feedback. It works by: (i) extending\n",
            "sparse binary feedback by training a satisfac-\n",
            "tion classiﬁer to label the unlabeled data; and\n",
            "(ii) training a reply corrector to map the bad\n",
            "replies to good ones. We ﬁnd that augment-\n",
            "ing training with model-corrected replies im-\n",
            "proves the ﬁnal dialogue model, and we can\n",
            "further improve performance by using both\n",
            "positive and negative replies through the re-\n",
            "cently proposed D IRECTOR model.\n",
            "1 Introduction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = create_vector_store(chunk)\n",
        "print(vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV955Q8Fedu3",
        "outputId": "06d24f75-3362-455b-b16f-01a7d60c909b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain_community.vectorstores.faiss.FAISS object at 0x7cc77edc79d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks = retrieve_relevant_chunks(vector_store, 'DEMO dataset', k=3)\n",
        "print(relevant_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GVDNEofjPT7",
        "outputId": "1a5f4a54-7520-4a1d-82e7-da9583d99cf9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sponses that are missing human labels; (3) next we\\nuse the reply corrector to correct bad bot responses\\n(lemons\\n ) into good ones, conditioning on hu-\\nman textual feedback; (4) ﬁnally we augment the\\ntraining data with the new good responses (cher-\\nryade\\n ) and re-train our ﬁnal dialogue models.\\nTo evaluate JUICER on state-of-the-art chatbots\\nin such a setting, we thus construct a new sparse\\nsampled version of the existing FITS dataset from\\nXu et al. (2022), which consists of fully annotated\\nhuman-model conversations between users and ex-\\nisting state-of-the-art internet-augmented models\\nsuch as BlenderBot 2 (Komeili et al., 2021; Xu\\net al., 2021) and SeeKeR (Shuster et al., 2022a).\\nWe explore a variety of methods to take advan-\\ntage of limited human feedback at each step of the\\nJUICER framework. Our main results are:arXiv:2210.15893v1  [cs.CL]  28 Oct 2022•We show that free-form textual feedback is\\na very useful signal for improving the per-', 'Table 1a shows the classiﬁers’ performance on the\\nFITS data and also their zero-shot performance on\\nDEMO.Adding the next human response helps. We\\nﬁnd the balanced accuracy of detecting satisfac-\\ntion using only the dialogue context and the bot\\nresponse itself is\\x1875% on FITS. It is signiﬁcantly\\nimproved to\\x1895% by including the next human\\nmessage in the input. A similar improvement is\\nfound when measuring balanced F1 as well. On the\\ndeployment dataset DEMO where organic users are\\nnot required to always write free-form textual feed-\\nback when seeing a bad reply, adding the human\\nresponse still improves the balanced F1 from 64.77\\nto 71.24, despite this being zero-shot performance\\n(without training on this dataset). These results\\nindicate the importance of using the next human\\nmessage to make satisfaction classiﬁcation deci-\\nsions. As using the next human response helps, we\\ndefault to using this satisfaction classiﬁer variant\\nin our standard J UICER setup.\\n6.2 Reply corrector', 'During conversations, crowdworkers click\\n or\\nper turn and give a ﬁnal rating (a score out of\\n5) in the end. We report the average good response\\nrate in percentage.5.1.2 DEMO\\nThe dataset DEMO is from the deployment of\\nBlenderBot 3 (Shuster et al., 2022b) with responses\\nveriﬁed by crowdworkers (Ju et al., 2022). In total\\n923 bot responses across 81 conversations are used\\nas an evaluation set.\\n5.2 Baselines\\nWe have two categories of baselines: (1) with-\\nout model-augmented data, and (2) with a prompt-\\nbased reply corrector . In addition, we also compare\\nwith oracle methods using 100% labeled feedback\\ndata without sampling.\\nBaselines without augmentation. The most\\nstraightforward baselines are to ﬁne-tune with the\\nlimited human-labeled feedback only.\\n•Gold corrections from 20% Gold correc-\\ntions provide a strong learning signal. Here,\\nwe simply ﬁne-tune BB2 3B on the 20% gold\\ncorrections.\\n•Free-form textual feedback from 20% Fol-\\nlowing Hancock et al. (2019), we ﬁne-tune']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# เพื่อดู prompt ใหม่\n",
        "def to_see_newprompt(query, relevant_chunks):\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    prompt = f\"\"\"Context information is below.\n",
        "    ---------------------\n",
        "    {' '.join(relevant_chunks)}\n",
        "    ---------------------\n",
        "    Given the context information and not prior knowledge, answer the question: {query}\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return prompt\n",
        "\n",
        "#################################################\n",
        "to_see_newprompt(\"DEMO dataset\",relevant_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "cGalaN5rtoDj",
        "outputId": "c9c628ea-6aba-4855-c8a9-e6535aebf4fd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context information is below.\\n    ---------------------\\n    sponses that are missing human labels; (3) next we\\nuse the reply corrector to correct bad bot responses\\n(lemons\\n ) into good ones, conditioning on hu-\\nman textual feedback; (4) ﬁnally we augment the\\ntraining data with the new good responses (cher-\\nryade\\n ) and re-train our ﬁnal dialogue models.\\nTo evaluate JUICER on state-of-the-art chatbots\\nin such a setting, we thus construct a new sparse\\nsampled version of the existing FITS dataset from\\nXu et al. (2022), which consists of fully annotated\\nhuman-model conversations between users and ex-\\nisting state-of-the-art internet-augmented models\\nsuch as BlenderBot 2 (Komeili et al., 2021; Xu\\net al., 2021) and SeeKeR (Shuster et al., 2022a).\\nWe explore a variety of methods to take advan-\\ntage of limited human feedback at each step of the\\nJUICER framework. Our main results are:arXiv:2210.15893v1  [cs.CL]  28 Oct 2022•We show that free-form textual feedback is\\na very useful signal for improving the per- Table 1a shows the classiﬁers’ performance on the\\nFITS data and also their zero-shot performance on\\nDEMO.Adding the next human response helps. We\\nﬁnd the balanced accuracy of detecting satisfac-\\ntion using only the dialogue context and the bot\\nresponse itself is\\x1875% on FITS. It is signiﬁcantly\\nimproved to\\x1895% by including the next human\\nmessage in the input. A similar improvement is\\nfound when measuring balanced F1 as well. On the\\ndeployment dataset DEMO where organic users are\\nnot required to always write free-form textual feed-\\nback when seeing a bad reply, adding the human\\nresponse still improves the balanced F1 from 64.77\\nto 71.24, despite this being zero-shot performance\\n(without training on this dataset). These results\\nindicate the importance of using the next human\\nmessage to make satisfaction classiﬁcation deci-\\nsions. As using the next human response helps, we\\ndefault to using this satisfaction classiﬁer variant\\nin our standard J UICER setup.\\n6.2 Reply corrector During conversations, crowdworkers click\\n or\\nper turn and give a ﬁnal rating (a score out of\\n5) in the end. We report the average good response\\nrate in percentage.5.1.2 DEMO\\nThe dataset DEMO is from the deployment of\\nBlenderBot 3 (Shuster et al., 2022b) with responses\\nveriﬁed by crowdworkers (Ju et al., 2022). In total\\n923 bot responses across 81 conversations are used\\nas an evaluation set.\\n5.2 Baselines\\nWe have two categories of baselines: (1) with-\\nout model-augmented data, and (2) with a prompt-\\nbased reply corrector . In addition, we also compare\\nwith oracle methods using 100% labeled feedback\\ndata without sampling.\\nBaselines without augmentation. The most\\nstraightforward baselines are to ﬁne-tune with the\\nlimited human-labeled feedback only.\\n•Gold corrections from 20% Gold correc-\\ntions provide a strong learning signal. Here,\\nwe simply ﬁne-tune BB2 3B on the 20% gold\\ncorrections.\\n•Free-form textual feedback from 20% Fol-\\nlowing Hancock et al. (2019), we ﬁne-tune\\n    ---------------------\\n    Given the context information and not prior knowledge, answer the question: DEMO dataset\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_gemini(\"DEMO dataset\",relevant_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "T43ZHXRZx1d8",
        "outputId": "6b425e65-49e9-45d4-ad6b-5762e8dc4bdd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The DEMO dataset is from the deployment of BlenderBot 3 (Shuster et al., 2022b) with responses verified by crowdworkers (Ju et al., 2022). In total 923 bot responses across 81 conversations are used as an evaluation set.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG pipeline รวม function ทั้งหมด\n",
        "def rag_pipeline(pdf_path, query):\n",
        "    # อ่าน PDF\n",
        "    text = read_pdf(pdf_path)\n",
        "\n",
        "    # แบ่งข้อความเป็นชิ้นเล็กๆ\n",
        "    chunks = split_text(text)\n",
        "\n",
        "    # สร้าง vector store\n",
        "    vector_store = create_vector_store(chunks)\n",
        "\n",
        "    # ค้นหาข้อมูลที่เกี่ยวข้อง\n",
        "    relevant_chunks = retrieve_relevant_chunks(vector_store, query)\n",
        "\n",
        "    # ถามคำถามและรับคำตอบจาก Gemini\n",
        "    answer = ask_gemini(query, relevant_chunks)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "hvbTGcyio25E"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ทดสอบ ใช้งานจริง\n",
        "pdf_path = \"/content/drive/MyDrive/อ้างอิง chatbot/when-life-gives-you-lemons-make-cherryade-converting-397dlsoh (2).pdf\"\n",
        "query = \"FITS dataset\"\n",
        "result = rag_pipeline(pdf_path, query)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aaG0yM4EpdOL",
        "outputId": "2a7f0c3b-a788-4d72-dbc6-f64b466a4031"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FITS dataset has 7768 train examples, 4245 valid examples, 9726 test examples, and 8907 unseen examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W31_M8djtqIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}