{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvEfLuuP32sZ6RW1gHW659",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tawanss/chatbot_project/blob/main/RAG_jason.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8my6CQqlOWM"
      },
      "outputs": [],
      "source": [
        " ติดตั้ง libraries ที่จำเป็น\n",
        "!pip install transformers datasets faiss-cpu torch\n",
        "\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import Dataset\n",
        "\n",
        "# โหลด JSON file\n",
        "with open('course_plan.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# แปลง JSON เป็น flat list ของ documents\n",
        "documents = []\n",
        "for year in data['coursePlan']:\n",
        "    for semester in year['semesters']:\n",
        "        if 'courses' in semester:\n",
        "            for course in semester['courses']:\n",
        "                doc = f\"Year {year['year']} Semester {semester['semester']}: {course['code']} - {course['name']} ({course['nameEn']}) - Credits: {course['credits']}\"\n",
        "                documents.append(doc)\n",
        "        elif 'plans' in semester:\n",
        "            for plan in semester['plans']:\n",
        "                for course in plan['courses']:\n",
        "                    doc = f\"Year {year['year']} Semester {semester['semester']} - {plan['name']}: {course['code']} - {course['name']} ({course['nameEn']}) - Credits: {course['credits']}\"\n",
        "                    documents.append(doc)\n",
        "\n",
        "# สร้าง Dataset\n",
        "dataset = Dataset.from_dict({\"text\": documents})\n",
        "\n",
        "# โหลด tokenizer และ model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# สร้าง function สำหรับ embedding\n",
        "def embed_text(examples):\n",
        "    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return {\"embeddings\": embeddings.numpy()}\n",
        "\n",
        "# สร้าง embeddings\n",
        "dataset = dataset.map(embed_text, batched=True, batch_size=16)\n",
        "\n",
        "# สร้าง FAISS index\n",
        "embeddings = np.array(dataset[\"embeddings\"]).astype(\"float32\")\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# Function สำหรับค้นหา\n",
        "def search(query, top_k=5):\n",
        "    query_vector = embed_text({\"text\": [query]})[\"embeddings\"][0]\n",
        "    distances, indices = index.search(np.array([query_vector]), top_k)\n",
        "    results = [{\"text\": dataset[i][\"text\"], \"distance\": distances[0][j]} for j, i in enumerate(indices[0])]\n",
        "    return results\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "query = \"วิชาภาษาอังกฤษ\"\n",
        "results = search(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"{i}. {result['text']} (Distance: {result['distance']:.4f})\")\n",
        "\n",
        "# Function สำหรับสร้าง prompt สำหรับ language model\n",
        "def generate_prompt(query, results):\n",
        "    prompt = f\"คำถาม: {query}\\n\\nข้อมูลที่เกี่ยวข้อง:\\n\"\n",
        "    for i, result in enumerate(results, 1):\n",
        "        prompt += f\"{i}. {result['text']}\\n\"\n",
        "    prompt += \"\\nกรุณาตอบคำถามโดยใช้ข้อมูลที่ให้มา:\"\n",
        "    return prompt\n",
        "\n",
        "# ตัวอย่างการสร้าง prompt\n",
        "prompt = generate_prompt(query, results)\n",
        "print(\"\\nGenerated Prompt:\")\n",
        "print(prompt)\n",
        "\n",
        "# หมายเหตุ: ในที่นี้เราไม่ได้เชื่อมต่อกับ language model จริง\n",
        "# คุณสามารถใช้ prompt นี้กับ model ของคุณเอง เช่น GPT-3 หรือ other LLMs\n",
        "# เพื่อสร้างคำตอบที่เหมาะสมต่อไป"
      ]
    }
  ]
}